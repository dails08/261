{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=master, master=local) created by __init__ at <ipython-input-2-f493162bd33f>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-f493162bd33f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'master'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ubuntu/spark-1.6.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m/home/ubuntu/spark-1.6.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway)\u001b[0m\n\u001b[0;32m    259\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 261\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    262\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=master, master=local) created by __init__ at <ipython-input-2-f493162bd33f>:3 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster('local').setAppName('master')\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\t{'C': 1}\r\n",
      "C\t{'B': 1}\r\n",
      "D\t{'A': 1, 'B': 1}\r\n",
      "E\t{'D': 1, 'B': 1, 'F': 1}\r\n",
      "F\t{'B': 1, 'E': 1}\r\n",
      "G\t{'B': 1, 'E': 1}\r\n",
      "H\t{'B': 1, 'E': 1}\r\n",
      "I\t{'B': 1, 'E': 1}\r\n",
      "J\t{'E': 1}\r\n",
      "K\t{'E': 1}\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./PageRank-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"./PageRank-test.txt\")\n",
    "\n",
    "#process data\n",
    "\n",
    "def processLine(line):\n",
    "    splits = line.strip().split('\\t')\n",
    "    return [splits[0], eval(splits[1])]\n",
    "data = raw_data.map(processLine).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count nodes\n",
    "def mapCount(entry):\n",
    "    yield (str(entry[0]), 1)\n",
    "    for key, value in entry[1].items():\n",
    "        yield (key, 1)\n",
    "        \n",
    "nodecount = data.flatMap(mapCount).reduceByKey(lambda x, y: 1).count()\n",
    "broadcast_nodecount = sc.broadcast(nodecount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 listed nodes.\n"
     ]
    }
   ],
   "source": [
    "print \"There are \" + str(broadcast_nodecount.value) + \" listed nodes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', [{}, 0.09090909090909091]), ('C', [{'B': 1}, 0.09090909090909091]), ('B', [{'C': 1}, 0.09090909090909091]), ('E', [{'B': 1, 'D': 1, 'F': 1}, 0.09090909090909091]), ('D', [{'A': 1, 'B': 1}, 0.09090909090909091]), ('G', [{'B': 1, 'E': 1}, 0.09090909090909091]), ('F', [{'B': 1, 'E': 1}, 0.09090909090909091]), ('I', [{'B': 1, 'E': 1}, 0.09090909090909091]), ('H', [{'B': 1, 'E': 1}, 0.09090909090909091]), ('K', [{'E': 1}, 0.09090909090909091]), ('J', [{'E': 1}, 0.09090909090909091])]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#prep data\n",
    "#nodes that don't have edges need to be better represented\n",
    "\n",
    "def prep_map(entry):\n",
    "    nodeID = entry[0]\n",
    "    edges = entry[1]\n",
    "    for edge, weight in edges.items():\n",
    "        yield (str(edge.strip()), {})\n",
    "    yield (str(nodeID), edges)\n",
    "    \n",
    "def prep_reduce(x, y):\n",
    "    edges = {}\n",
    "    for key, value in x.items():\n",
    "        edges[key] = value\n",
    "    for key, value in y.items():\n",
    "        edges[key] = value\n",
    "    return edges\n",
    "\n",
    "total_pr = sc.accumulator(0.0)\n",
    "\n",
    "def init_entry(entry):\n",
    "    total_pr.add(1.0 / broadcast_nodecount.value)\n",
    "    return (entry[0], [entry[1], 1.0 / broadcast_nodecount.value])\n",
    "\n",
    "prepped_data = data.flatMap(prep_map).reduceByKey(prep_reduce).map(init_entry).cache()\n",
    "\n",
    "print prepped_data.collect()\n",
    "\n",
    "print total_pr.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "accum_mass_moved = sc.accumulator(0)\n",
    "\n",
    "broadcast_damping_factor = sc.broadcast(.15)\n",
    "broadcast_dangling_mass = sc.broadcast(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phaseOneMapper(entry):\n",
    "    label = entry[0]\n",
    "    edges = entry[1][0]\n",
    "    pr = entry[1][1]\n",
    "\n",
    "    if len(edges) == 0:\n",
    "        accum_dangling_mass.add(pr)\n",
    "    else:\n",
    "        forwarding_pr = pr / len(edges)\n",
    "        for edge, weight in edges.items():\n",
    "            yield (edge, [{}, forwarding_pr])\n",
    "    yield (label, [edges, 0.0])\n",
    "\n",
    "def phaseOneReducer(x, y):\n",
    "    edges = {}\n",
    "    for edge, weight in x[0].items():\n",
    "        edges[edge] = weight\n",
    "    for edge, weight in y[0].items():\n",
    "        edges[edge] = weight\n",
    "    return [edges, x[1] + y[1]]\n",
    "\n",
    "def deserialize(entry):\n",
    "    label = entry[1:-1].split(',',1)[0][1:-1]\n",
    "    pair = eval(entry[1:-1].split(',',1)[1])\n",
    "    edges = pair[0]\n",
    "    pr = pair[1]\n",
    "    return (label, [edges, pr])\n",
    "\n",
    "\n",
    "def finalize(entry, dangling_mass):\n",
    "    label = str(entry[0])\n",
    "    edges = entry[1][0]\n",
    "    pr = entry[1][1]\n",
    "\n",
    "    pr_prime = broadcast_damping_factor.value * (1.0 / broadcast_nodecount.value) + \\\n",
    "    (1 - broadcast_damping_factor.value) * (dangling_mass / broadcast_nodecount.value + pr)\n",
    "\n",
    "    return (label, [edges, pr_prime])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', [{}, .045])\n",
      "'A', [{}, .045]\n",
      "[\"'A'\", ' [{}, .045]']\n",
      "'A'\n",
      "A\n",
      "[{}, 0.045]\n"
     ]
    }
   ],
   "source": [
    "testString = \"('A', [{}, .045])\"\n",
    "print testString\n",
    "print testString[1:-1]\n",
    "print testString[1:-1].split(',',1)\n",
    "print testString[1:-1].split(',',1)[0]\n",
    "print testString[1:-1].split(',',1)[0][1:-1]\n",
    "print eval(testString[1:-1].split(',',1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', [{}, 0.0]), ('B', [{}, 0.09090909090909091]), ('C', [{'B': 1}, 0.0]), ('C', [{}, 0.09090909090909091]), ('B', [{'C': 1}, 0.0]), ('B', [{}, 0.030303030303030304]), ('D', [{}, 0.030303030303030304]), ('F', [{}, 0.030303030303030304]), ('E', [{'B': 1, 'D': 1, 'F': 1}, 0.0]), ('A', [{}, 0.045454545454545456]), ('B', [{}, 0.045454545454545456]), ('D', [{'A': 1, 'B': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('G', [{'B': 1, 'E': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('F', [{'B': 1, 'E': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('I', [{'B': 1, 'E': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('H', [{'B': 1, 'E': 1}, 0.0]), ('E', [{}, 0.09090909090909091]), ('K', [{'E': 1}, 0.0]), ('E', [{}, 0.09090909090909091]), ('J', [{'E': 1}, 0.0])]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#initial testing\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "\n",
    "broadcast_damping_factor = sc.broadcast(.15)\n",
    "\n",
    "print prepped_data.flatMap(phaseOneMapper).collect()\n",
    "\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "\n",
    "\n",
    "print prepped_data.flatMap(phaseOneMapper).map(lambda x: x[1][1]).reduce(lambda x, y: x + y) + \\\n",
    "    accum_dangling_mass.value\n",
    "\n",
    "\n",
    "#                 .reduceByKey(phaseOneReducer)\\\n",
    "#                 .map(lambda x: finalize(x, accum_dangling_mass.value))\\\n",
    "#                 .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', [{}, 0.0]), ('B', [{}, 0.09090909090909091]), ('C', [{'B': 1}, 0.0]), ('C', [{}, 0.09090909090909091]), ('B', [{'C': 1}, 0.0]), ('B', [{}, 0.030303030303030304]), ('D', [{}, 0.030303030303030304]), ('F', [{}, 0.030303030303030304]), ('E', [{'B': 1, 'D': 1, 'F': 1}, 0.0]), ('A', [{}, 0.045454545454545456]), ('B', [{}, 0.045454545454545456]), ('D', [{'A': 1, 'B': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('G', [{'B': 1, 'E': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('F', [{'B': 1, 'E': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('I', [{'B': 1, 'E': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('H', [{'B': 1, 'E': 1}, 0.0]), ('E', [{}, 0.09090909090909091]), ('K', [{'E': 1}, 0.0]), ('E', [{}, 0.09090909090909091]), ('J', [{'E': 1}, 0.0])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#phase two testing\n",
    "\n",
    "\n",
    "broadcast_damping_factor = sc.broadcast(.15)\n",
    "\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "\n",
    "print prepped_data.flatMap(phaseOneMapper).collect()\n",
    "\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "\n",
    "\n",
    "prepped_data.flatMap(phaseOneMapper).reduceByKey(phaseOneReducer).collect()\n",
    "#                 .map(lambda x: finalize(x, accum_dangling_mass.value))\\\n",
    "#                 .collect()\n",
    "\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "prepped_data.flatMap(phaseOneMapper)\\\n",
    "    .reduceByKey(phaseOneReducer)\\\n",
    "    .map(lambda x: x[1][1])\\\n",
    "    .reduce(lambda x, y: x + y)\\\n",
    "    + accum_dangling_mass.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', [{}, 0.0]), ('B', [{}, 0.09090909090909091]), ('C', [{'B': 1}, 0.0]), ('C', [{}, 0.09090909090909091]), ('B', [{'C': 1}, 0.0]), ('B', [{}, 0.030303030303030304]), ('D', [{}, 0.030303030303030304]), ('F', [{}, 0.030303030303030304]), ('E', [{'B': 1, 'D': 1, 'F': 1}, 0.0]), ('A', [{}, 0.045454545454545456]), ('B', [{}, 0.045454545454545456]), ('D', [{'A': 1, 'B': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('G', [{'B': 1, 'E': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('F', [{'B': 1, 'E': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('I', [{'B': 1, 'E': 1}, 0.0]), ('B', [{}, 0.045454545454545456]), ('E', [{}, 0.045454545454545456]), ('H', [{'B': 1, 'E': 1}, 0.0]), ('E', [{}, 0.09090909090909091]), ('K', [{'E': 1}, 0.0]), ('E', [{}, 0.09090909090909091]), ('J', [{'E': 1}, 0.0])]\n",
      "('A', [{}, 0.045454545454545456])\n",
      "('C', [{'B': 1}, 0.09090909090909091])\n",
      "('B', [{'C': 1}, 0.34848484848484856])\n",
      "('E', [{'B': 1, 'D': 1, 'F': 1}, 0.36363636363636365])\n",
      "('D', [{'A': 1, 'B': 1}, 0.030303030303030304])\n",
      "('G', [{'B': 1, 'E': 1}, 0.0])\n",
      "('F', [{'B': 1, 'E': 1}, 0.030303030303030304])\n",
      "('I', [{'B': 1, 'E': 1}, 0.0])\n",
      "('H', [{'B': 1, 'E': 1}, 0.0])\n",
      "('K', [{'E': 1}, 0.0])\n",
      "('J', [{'E': 1}, 0.0])\n",
      "[('A', [{}, 0.059297520661157024]), ('C', [{'B': 1}, 0.09793388429752066]), ('B', [{'C': 1}, 0.31687327823691464]), ('E', [{'B': 1, 'D': 1, 'F': 1}, 0.32975206611570246]), ('D', [{'A': 1, 'B': 1}, 0.046418732782369146]), ('G', [{'B': 1, 'E': 1}, 0.02066115702479339]), ('F', [{'B': 1, 'E': 1}, 0.046418732782369146]), ('I', [{'B': 1, 'E': 1}, 0.02066115702479339]), ('H', [{'B': 1, 'E': 1}, 0.02066115702479339]), ('K', [{'E': 1}, 0.02066115702479339]), ('J', [{'E': 1}, 0.02066115702479339])]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#full pipeline testing\n",
    "\n",
    "\n",
    "broadcast_damping_factor = sc.broadcast(.15)\n",
    "\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "\n",
    "print prepped_data.flatMap(phaseOneMapper).collect()\n",
    "\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "\n",
    "! rm -rf ./intermediate/\n",
    "prepped_data.flatMap(phaseOneMapper).reduceByKey(phaseOneReducer).saveAsTextFile(\"./intermediate\")\n",
    "!cat intermediate/part-00000\n",
    "\n",
    "danging_mass = accum_dangling_mass.value\n",
    "\n",
    "intermediate_data = sc.textFile(\"./intermediate/part-00000\").map(deserialize)\\\n",
    "                .map(lambda x: finalize(x, danging_mass))\n",
    "    \n",
    "print intermediate_data.collect()\n",
    "    \n",
    "total_pr =  intermediate_data.map(lambda x: x[1][1]).reduce(lambda x, y: x + y)\n",
    "        \n",
    "\n",
    "\n",
    "print total_pr\n",
    "\n",
    "# accum_dangling_mass = sc.accumulator(0.0)\n",
    "# prepped_data.flatMap(phaseOneMapper)\\\n",
    "#     .reduceByKey(phaseOneReducer)\\\n",
    "#     .map(lambda x: x[1][1])\\\n",
    "#     .reduce(lambda x, y: x + y)\\\n",
    "#     + accum_dangling_mass.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
