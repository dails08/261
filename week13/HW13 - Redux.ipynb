{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f0e2c979110>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5781290\n",
      "Time taken: 23.9189379215\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print sc.textFile(\"/data/all-pages-indexed-out.txt\").map(lambda x: 1).reduce(lambda x, y: x + y)\n",
    "print \"Time taken: \" + str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5781290\n",
      "Time taken: 9.1845138073\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print sc.textFile(\"/data/all-pages-indexed-out.txt\").map(lambda x: 1).reduce(lambda x, y: x + y)\n",
    "print \"Time taken: \" + str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapCount is for counting the true number of nodes\n",
    "# necessary since nodes with no outgoing links are not listed on their own line\n",
    "\n",
    "def mapCount(entry):\n",
    "    yield (str(entry[0]), 1)\n",
    "    for key, value in entry[1].items():\n",
    "        yield (key, 1)\n",
    "        \n",
    "# processLine turns the string into python objects,\n",
    "# specifically the label and a dictionary of edges.\n",
    "# this assumes zero formatting errors\n",
    "def processLine(line):\n",
    "    splits = line.strip().split('\\t')\n",
    "    return [splits[0], eval(splits[1])]\n",
    "\n",
    "# prep_map generates a line for each possible node, including nodes that exist\n",
    "# only in the edge dictionary of another node\n",
    "def prep_map(entry):\n",
    "    nodeID = entry[0]\n",
    "    edges = entry[1]\n",
    "    for edge, weight in edges.items():\n",
    "        yield (str(edge.strip()), {})\n",
    "    yield (str(nodeID), edges)\n",
    "    \n",
    "# prep_reduce combines the dictionaries of the records created in the map phase\n",
    "def prep_reduce(x, y):\n",
    "    edges = {}\n",
    "    for key, value in x.items():\n",
    "        edges[key] = value\n",
    "    for key, value in y.items():\n",
    "        edges[key] = value\n",
    "    return edges\n",
    "\n",
    "# init_entry distributes the initial probability mass\n",
    "# it also adds a value into the record of 0.0 which will eventually represent\n",
    "# the amount of change in that node's probability mass\n",
    "# which we'll use to check for convergence\n",
    "# it is reliant on the accum_total_pr accumulator for its closure\n",
    "\n",
    "accum_total_pr = sc.accumulator(0.0)\n",
    "\n",
    "def init_entry(entry):\n",
    "    accum_total_pr.add(1.0 / broadcast_nodecount.value)\n",
    "    return (entry[0], [entry[1], 1.0 / broadcast_nodecount.value, 0.0]) #the 0.0 is the delta pr \n",
    "\n",
    "\n",
    "# Phase one consists of a map and reducebykey phase.\n",
    "# phaseOneMapper stores the pr mass of the node as it is as the beginning of the step\n",
    "# if there are no outgoing edges, it passes all the pr mass of that node to the dangling_mass accumulator\n",
    "# which is defined here for the closure.\n",
    "# if there are edges, it divides the pr mass evenly among them and emits the target node with \n",
    "# an empty edge list and a zero previous pr mass.  This is done so that when records are combined\n",
    "# in the reduce step, there will be exactly one record with a full edge list and an accurate\n",
    "# previous pr mass, so we can cummutatively and associatively add the records together\n",
    "# and arrive at a single accurate record\n",
    "\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "\n",
    "\n",
    "def phaseOneMapper(entry):\n",
    "    label = entry[0]\n",
    "    edges = entry[1][0]\n",
    "    pr = entry[1][1]\n",
    "    previous = pr\n",
    "\n",
    "    if len(edges) == 0:\n",
    "        accum_dangling_mass.add(pr)\n",
    "    else:\n",
    "        forwarding_pr = pr / len(edges)\n",
    "        for edge, weight in edges.items():\n",
    "            yield (edge, [{}, forwarding_pr, 0.0])\n",
    "    yield (label, [edges, 0.0, pr])\n",
    "\n",
    "def phaseOneReducer(x, y):\n",
    "    edges = {}\n",
    "    for edge, weight in x[0].items():\n",
    "        edges[edge] = weight\n",
    "    for edge, weight in y[0].items():\n",
    "        edges[edge] = weight\n",
    "    return [edges, x[1] + y[1], x[2] + y[2]]\n",
    "\n",
    "# the finalize state distributes the dangling pr mass by knowing from the broadcast variables how many\n",
    "# nodes there are and by being passed the dangling mass from a variable defined with the accumulator\n",
    "# in the driver program logic.  It also calculates the final pr mass of the node by the pagerank equation\n",
    "# additionally, it records the absolute value of how much the pr mass of this node has changed and\n",
    "# passes it to the accum_moved_mass accumulator.  \n",
    "# Since the original pr mass value is no longer needed, it is not stored\n",
    "# (recall that it is regenerated in the phase one map step and is simply the value here called pr_prime)\n",
    "\n",
    "accum_moved_mass = sc.accumulator(0.0)\n",
    "\n",
    "def finalize(entry, dangling_mass):\n",
    "    label = str(entry[0])\n",
    "    edges = entry[1][0]\n",
    "    pr = entry[1][1]\n",
    "    previous = entry[1][2]\n",
    "    \n",
    "    pr_prime = broadcast_damping_factor.value * (1.0 / broadcast_nodecount.value) + \\\n",
    "    (1 - broadcast_damping_factor.value) * (dangling_mass / broadcast_nodecount.value + pr)\n",
    "    \n",
    "    accum_moved_mass.add(((pr_prime - previous)**2)**.5)\n",
    "    \n",
    "    return (label, [edges, pr_prime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    if seconds < 60:\n",
    "        return str(seconds) + ' s'\n",
    "    elif seconds < 60*60:\n",
    "        return str(seconds/60) + ' m'\n",
    "    return str(seconds/60/60) + ' h'\n",
    "\n",
    "#using the book's technique\n",
    "def PageRank(filename, repartition = True, verbose = False):\n",
    "\n",
    "    verbose = False\n",
    "\n",
    "    #load the data and process it\n",
    "#     data = sc.textFile(\"/data/all-pages-indexed-out.txt\").map(processLine).repartition(50).cache()\n",
    "    # data = sc.textFile(\"/data/PageRank-test.txt\").map(processLine)\n",
    "    if repartition:\n",
    "        data = sc.textFile(filename).map(processLine).repartition(25)\n",
    "    else:\n",
    "        data = sc.textFile(filename).map(processLine)\n",
    "\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # count the true number of nodes\n",
    "    print \"Starting to do smart count\"\n",
    "    start_time = time.time()\n",
    "    nodecount = data.flatMap(mapCount).reduceByKey(lambda x, y: 1).count()\n",
    "    nodecount_time = time.time() - start_time\n",
    "    broadcast_nodecount = sc.broadcast(nodecount)\n",
    "\n",
    "    print \"There are \" + str(broadcast_nodecount.value) + \" nodes in the dataset\"\n",
    "    print \"(\" + format_time(nodecount_time) + \")\\n\"\n",
    "\n",
    "    \n",
    "    broadcast_damping_factor = sc.broadcast(.15)\n",
    "\n",
    "    # conduct the initial preparations\n",
    "    prepped_data = data.flatMap(prep_map).reduceByKey(prep_reduce).cache()\n",
    "    \n",
    "    ranks = prepped_data.map(lambda x: (x[0], 1.0))\n",
    "    \n",
    "    def distribute(baseTuple):\n",
    "        pageID = baseTuple[0]\n",
    "        values = baseTuple[1]\n",
    "        for link , weight in values[0].items():\n",
    "            yield (link, values[1]/len(values[0]))\n",
    "    \n",
    "    print \"Starting to loop\"\n",
    "    looping_start_time = time.time()\n",
    "    for i in range(10):\n",
    "        print \"Starting iteration \" + str(i + 1)\n",
    "        iteration_start_time = time.time()\n",
    "        ranks = prepped_data.join(ranks).flatMap(distribute).reduceByKey(lambda x, y: x + y).mapValues(lambda x: .15 + .85*x)\n",
    "        ranks.count()\n",
    "        iteration_duration = time.time() - iteration_start_time\n",
    "        print \"Iteration \" + str(i + 1) + \" took \" + format_time(iteration_duration)\n",
    "        \n",
    "    looping_duration = time.time() - looping_start_time\n",
    "    print \"Looping took \" + format_time(looping_duration)\n",
    "    print ranks.takeOrdered(10, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to do smart count\n",
      "There are 15192277 nodes in the dataset\n",
      "(2.34309776624 m)\n",
      "\n",
      "Starting to loop\n",
      "Starting iteration 1\n",
      "Iteration 1 took 10.3880624493 m\n",
      "Starting iteration 2\n",
      "Iteration 2 took 5.3471416831 m\n",
      "Starting iteration 3\n",
      "Iteration 3 took 3.94448184967 m\n",
      "Starting iteration 4\n",
      "Iteration 4 took 3.19412236611 m\n",
      "Starting iteration 5\n",
      "Iteration 5 took 4.32671005328 m\n",
      "Starting iteration 6\n",
      "Iteration 6 took 2.97266261578 m\n",
      "Starting iteration 7\n",
      "Iteration 7 took 4.3059970657 m\n",
      "Starting iteration 8\n",
      "Iteration 8 took 3.90866698424 m\n",
      "Starting iteration 9\n",
      "Iteration 9 took 3.25100076596 m\n",
      "Starting iteration 10\n",
      "Iteration 10 took 4.24620841742 m\n",
      "Looping took 45.88507665 m\n",
      "[('5704503', 0.1500232217340775), ('1264900', 0.1500232217340775), ('11557656', 0.1500232217340775), ('691874', 0.1500232217340775), ('1325013', 0.1500232217340775), ('3000006', 0.1500232217340775), ('2916454', 0.1500232217340775), ('6008884', 0.1500232217340775), ('10791480', 0.1500232217340775), ('12514353', 0.1500232217340775)]\n"
     ]
    }
   ],
   "source": [
    "PageRank(\"/data/all-pages-indexed-out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
