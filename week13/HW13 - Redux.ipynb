{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f3c880871d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5781290\n",
      "Time taken: 23.9189379215\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print sc.textFile(\"/data/all-pages-indexed-out.txt\").map(lambda x: 1).reduce(lambda x, y: x + y)\n",
    "print \"Time taken: \" + str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5781290\n",
      "Time taken: 9.1845138073\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print sc.textFile(\"/data/all-pages-indexed-out.txt\").map(lambda x: 1).reduce(lambda x, y: x + y)\n",
    "print \"Time taken: \" + str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapCount is for counting the true number of nodes\n",
    "# necessary since nodes with no outgoing links are not listed on their own line\n",
    "\n",
    "def mapCount(entry):\n",
    "    yield (str(entry[0]), 1)\n",
    "    for key, value in entry[1].items():\n",
    "        yield (key, 1)\n",
    "        \n",
    "# processLine turns the string into python objects,\n",
    "# specifically the label and a dictionary of edges.\n",
    "# this assumes zero formatting errors\n",
    "def processLine(line):\n",
    "    splits = line.strip().split('\\t')\n",
    "    return [splits[0], eval(splits[1])]\n",
    "\n",
    "# prep_map generates a line for each possible node, including nodes that exist\n",
    "# only in the edge dictionary of another node\n",
    "def prep_map(entry):\n",
    "    nodeID = entry[0]\n",
    "    edges = entry[1]\n",
    "    for edge, weight in edges.items():\n",
    "        yield (str(edge.strip()), {})\n",
    "    yield (str(nodeID), edges)\n",
    "    \n",
    "# prep_reduce combines the dictionaries of the records created in the map phase\n",
    "def prep_reduce(x, y):\n",
    "    edges = {}\n",
    "    for key, value in x.items():\n",
    "        edges[key] = value\n",
    "    for key, value in y.items():\n",
    "        edges[key] = value\n",
    "    return edges\n",
    "\n",
    "# init_entry distributes the initial probability mass\n",
    "# it also adds a value into the record of 0.0 which will eventually represent\n",
    "# the amount of change in that node's probability mass\n",
    "# which we'll use to check for convergence\n",
    "# it is reliant on the accum_total_pr accumulator for its closure\n",
    "\n",
    "accum_total_pr = sc.accumulator(0.0)\n",
    "\n",
    "def init_entry(entry):\n",
    "    accum_total_pr.add(1.0 / broadcast_nodecount.value)\n",
    "    return (entry[0], [entry[1], 1.0 / broadcast_nodecount.value, 0.0]) #the 0.0 is the delta pr \n",
    "\n",
    "\n",
    "# Phase one consists of a map and reducebykey phase.\n",
    "# phaseOneMapper stores the pr mass of the node as it is as the beginning of the step\n",
    "# if there are no outgoing edges, it passes all the pr mass of that node to the dangling_mass accumulator\n",
    "# which is defined here for the closure.\n",
    "# if there are edges, it divides the pr mass evenly among them and emits the target node with \n",
    "# an empty edge list and a zero previous pr mass.  This is done so that when records are combined\n",
    "# in the reduce step, there will be exactly one record with a full edge list and an accurate\n",
    "# previous pr mass, so we can cummutatively and associatively add the records together\n",
    "# and arrive at a single accurate record\n",
    "\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "\n",
    "\n",
    "def phaseOneMapper(entry):\n",
    "    label = entry[0]\n",
    "    edges = entry[1][0]\n",
    "    pr = entry[1][1]\n",
    "    previous = pr\n",
    "\n",
    "    if len(edges) == 0:\n",
    "        accum_dangling_mass.add(pr)\n",
    "    else:\n",
    "        forwarding_pr = pr / len(edges)\n",
    "        for edge, weight in edges.items():\n",
    "            yield (edge, [{}, forwarding_pr, 0.0])\n",
    "    yield (label, [edges, 0.0, pr])\n",
    "\n",
    "def phaseOneReducer(x, y):\n",
    "    edges = {}\n",
    "    for edge, weight in x[0].items():\n",
    "        edges[edge] = weight\n",
    "    for edge, weight in y[0].items():\n",
    "        edges[edge] = weight\n",
    "    return [edges, x[1] + y[1], x[2] + y[2]]\n",
    "\n",
    "# the finalize state distributes the dangling pr mass by knowing from the broadcast variables how many\n",
    "# nodes there are and by being passed the dangling mass from a variable defined with the accumulator\n",
    "# in the driver program logic.  It also calculates the final pr mass of the node by the pagerank equation\n",
    "# additionally, it records the absolute value of how much the pr mass of this node has changed and\n",
    "# passes it to the accum_moved_mass accumulator.  \n",
    "# Since the original pr mass value is no longer needed, it is not stored\n",
    "# (recall that it is regenerated in the phase one map step and is simply the value here called pr_prime)\n",
    "\n",
    "accum_moved_mass = sc.accumulator(0.0)\n",
    "\n",
    "def finalize(entry, dangling_mass):\n",
    "    label = str(entry[0])\n",
    "    edges = entry[1][0]\n",
    "    pr = entry[1][1]\n",
    "    previous = entry[1][2]\n",
    "    \n",
    "    pr_prime = broadcast_damping_factor.value * (1.0 / broadcast_nodecount.value) + \\\n",
    "    (1 - broadcast_damping_factor.value) * (dangling_mass / broadcast_nodecount.value + pr)\n",
    "    \n",
    "    accum_moved_mass.add(((pr_prime - previous)**2)**.5)\n",
    "    \n",
    "    return (label, [edges, pr_prime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    if seconds < 60:\n",
    "        return str(seconds) + ' s'\n",
    "    elif seconds < 60*60:\n",
    "        return str(seconds/60) + ' m'\n",
    "    return str(seconds/60/60) + ' h'\n",
    "\n",
    "#using the book's technique\n",
    "def PageRank(filename, repartition = True, verbose = False, iterations = 10):\n",
    "\n",
    "    verbose = False\n",
    "\n",
    "    #load the data and process it\n",
    "#     data = sc.textFile(\"/data/all-pages-indexed-out.txt\").map(processLine).repartition(50).cache()\n",
    "    # data = sc.textFile(\"/data/PageRank-test.txt\").map(processLine)\n",
    "    if repartition:\n",
    "        data = sc.textFile(filename).map(processLine).repartition(25)\n",
    "    else:\n",
    "        data = sc.textFile(filename).map(processLine)\n",
    "\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # count the true number of nodes\n",
    "    print \"Starting to do smart count\"\n",
    "    start_time = time.time()\n",
    "    nodecount = data.flatMap(mapCount).reduceByKey(lambda x, y: 1).count()\n",
    "    nodecount_time = time.time() - start_time\n",
    "    broadcast_nodecount = sc.broadcast(nodecount)\n",
    "\n",
    "    print \"There are \" + str(broadcast_nodecount.value) + \" nodes in the dataset\"\n",
    "    print \"(\" + format_time(nodecount_time) + \")\\n\"\n",
    "\n",
    "    \n",
    "    broadcast_damping_factor = sc.broadcast(.15)\n",
    "\n",
    "    # conduct the initial preparations\n",
    "    prepped_data = data.flatMap(prep_map).reduceByKey(prep_reduce).cache()\n",
    "    \n",
    "    ranks = prepped_data.map(lambda x: (x[0], 1.0))\n",
    "    \n",
    "    def distribute(baseTuple):\n",
    "        pageID = baseTuple[0]\n",
    "        values = baseTuple[1]\n",
    "        for link , weight in values[0].items():\n",
    "            yield (link, values[1]/len(values[0]))\n",
    "    \n",
    "    print \"Starting to loop\"\n",
    "    looping_start_time = time.time()\n",
    "    for i in range(iterations):\n",
    "        print \"Starting iteration \" + str(i + 1)\n",
    "        iteration_start_time = time.time()\n",
    "        ranks = prepped_data.join(ranks).flatMap(distribute).reduceByKey(lambda x, y: x + y).mapValues(lambda x: .15 + .85*x)\n",
    "        ranks.count()\n",
    "        iteration_duration = time.time() - iteration_start_time\n",
    "        print \"Iteration \" + str(i + 1) + \" took \" + format_time(iteration_duration)\n",
    "        \n",
    "    looping_duration = time.time() - looping_start_time\n",
    "    print \"Looping took \" + format_time(looping_duration)\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to do smart count\n",
      "There are 15192277 nodes in the dataset\n",
      "(3.76208531857 m)\n",
      "\n",
      "Starting to loop\n",
      "Starting iteration 1\n",
      "Iteration 1 took 11.654529798 m\n",
      "Starting iteration 2\n",
      "Iteration 2 took 6.541360418 m\n",
      "Starting iteration 3\n",
      "Iteration 3 took 3.95402544737 m\n",
      "Starting iteration 4\n",
      "Iteration 4 took 3.48739026785 m\n",
      "Starting iteration 5\n",
      "Iteration 5 took 2.76645898422 m\n",
      "Starting iteration 6\n",
      "Iteration 6 took 3.88560298284 m\n",
      "Starting iteration 7\n",
      "Iteration 7 took 2.77663143476 m\n",
      "Starting iteration 8\n",
      "Iteration 8 took 3.80538539886 m\n",
      "Starting iteration 9\n",
      "Iteration 9 took 3.85229249795 m\n",
      "Starting iteration 10\n",
      "Iteration 10 took 3.96437961658 m\n",
      "Looping took 46.6880789518 m\n",
      "[('5704503', 0.1500232217340775), ('1264900', 0.1500232217340775), ('11557656', 0.1500232217340775), ('691874', 0.1500232217340775), ('1325013', 0.1500232217340775), ('3000006', 0.1500232217340775), ('2916454', 0.1500232217340775), ('7421268', 0.1500232217340775), ('6008884', 0.1500232217340775), ('12514353', 0.1500232217340775), ('3000007', 0.1500232217340775), ('11957544', 0.1500232217340775), ('10791480', 0.1500232217340775), ('9445515', 0.1500232217340775), ('14298567', 0.1500232217340775), ('1325015', 0.1500232217340775), ('11957545', 0.1500232217340775), ('10791481', 0.1500232217340775), ('12514352', 0.1500232217340775), ('1325014', 0.1500232217340775), ('746787', 0.1500232217340775), ('3000001', 0.1500232217340775), ('2916453', 0.1500232217340775), ('12469207', 0.1500232217340775), ('14298565', 0.1500232217340775), ('1325017', 0.1500232217340775), ('746784', 0.1500232217340775), ('12468191', 0.1500232217340775), ('13210046', 0.1500232217340775), ('10597784', 0.1500232217340775), ('12468190', 0.1500232217340775), ('1039684', 0.1500232217340775), ('1040454', 0.1500232217340775), ('1325016', 0.1500232217340775), ('3000003', 0.1500232217340775), ('13200419', 0.1500232217340775), ('8004881', 0.1500232217340775), ('4892278', 0.1500232217340775), ('10638148', 0.1500232217340775), ('4056306', 0.1500232217340775), ('11578006', 0.1500232217340775), ('10597780', 0.1500232217340775), ('9839396', 0.1500232217340775), ('7421260', 0.1500232217340775), ('1040450', 0.1500232217340775), ('13200415', 0.1500232217340775), ('1039666', 0.1500232217340775), ('10032329', 0.1500232217340775), ('10958753', 0.1500232217340775), ('7665543', 0.1500232217340775), ('13602089', 0.1500232217340775), ('7582860', 0.1500232217340775), ('2641857', 0.1500232217340775), ('11578005', 0.1500232217340775), ('1039667', 0.1500232217340775), ('7421262', 0.1500232217340775), ('10958752', 0.1500232217340775), ('10217486', 0.1500232217340775), ('2641856', 0.1500232217340775), ('2876377', 0.1500232217340775), ('9839392', 0.1500232217340775), ('2641855', 0.1500232217340775), ('10954162', 0.1500232217340775), ('9839393', 0.1500232217340775), ('11557665', 0.1500232217340775), ('10638141', 0.1500232217340775), ('8005202', 0.1500232217340775), ('2641854', 0.1500232217340775), ('7582857', 0.1500232217340775), ('2641853', 0.1500232217340775), ('2476346', 0.1500232217340775), ('10958756', 0.1500232217340775), ('1297931', 0.1500232217340775), ('1281286', 0.1500232217340775), ('8005200', 0.1500232217340775), ('1040459', 0.1500232217340775), ('4320698', 0.1500232217340775), ('746788', 0.1500232217340775), ('13602087', 0.1500232217340775), ('14298568', 0.1500232217340775), ('8005201', 0.1500232217340775), ('2641851', 0.1500232217340775), ('10377872', 0.1500232217340775), ('2876378', 0.1500232217340775), ('5622704', 0.1500232217340775), ('13495014', 0.1500232217340775), ('2641850', 0.1500232217340775), ('10638145', 0.1500232217340775), ('2876379', 0.1500232217340775), ('750188', 0.1500232217340775), ('3257260', 0.1500232217340775), ('11136108', 0.1500232217340775), ('10413531', 0.1500232217340775), ('3147406', 0.1500232217340775), ('3321842', 0.1500232217340775), ('3184352', 0.1500232217340775), ('3147404', 0.1500232217340775), ('9839398', 0.1500232217340775), ('3184351', 0.1500232217340775), ('3723389', 0.1500232217340775)]\n"
     ]
    }
   ],
   "source": [
    "results = PageRank(\"/data/all-pages-indexed-out.txt\")\n",
    "results_ten_iters = results.takeOrdered(100, key = lambda x: x[1])\n",
    "print results_ten_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to do smart count\n",
      "There are 15192277 nodes in the dataset\n",
      "(1.6937171181 m)\n",
      "\n",
      "Starting to loop\n",
      "Starting iteration 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d4ff3993f94d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults_100_iters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPageRank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/data/all-pages-indexed-out.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtakeOrdered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-40b1792c21e9>\u001b[0m in \u001b[0;36mPageRank\u001b[1;34m(filename, repartition, verbose, iterations)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0miteration_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepped_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mranks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m.15\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m.85\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mranks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0miteration_duration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0miteration_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Iteration \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" took \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mformat_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration_duration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \"\"\"\n\u001b[1;32m-> 1004\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    993\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m         \"\"\"\n\u001b[1;32m--> 995\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \"\"\"\n\u001b[0;32m    770\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m    813\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    432\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_100_iters = PageRank(\"/data/all-pages-indexed-out.txt\", iterations = 50)\n",
    "print results_100_iters.takeOrdered(100, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
