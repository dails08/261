{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 13\n",
    "### Thomas Atkins, Chris Dailey, Sam Kahn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1\n",
    "Write a basic Spark implementation of the iterative PageRank algorithm\n",
    "that takes sparse adjacency lists as input.\n",
    "Make sure that your implementation utilizes teleportation (1-damping/the number of nodes in the network), \n",
    "and further, distributes the mass of dangling nodes with each iteration\n",
    "so that the output of each iteration is correctly normalized (sums to 1).\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page,\n",
    "chooses the next page to which it will move by clicking at random, with probability d,\n",
    "one of the hyperlinks in the current page. This probability is represented by a so-called\n",
    "‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer\n",
    "jumps to any web page in the network. If a page is a dangling end, meaning it has no\n",
    "outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform\n",
    "distribution and “teleports” to that page]\n",
    "\n",
    "In your Spark solution, please use broadcast variables and caching to make sure your code is as efficient as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster('local').setAppName('master')\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\t{'C': 1}\r\n",
      "C\t{'B': 1}\r\n",
      "D\t{'A': 1, 'B': 1}\r\n",
      "E\t{'D': 1, 'B': 1, 'F': 1}\r\n",
      "F\t{'B': 1, 'E': 1}\r\n",
      "G\t{'B': 1, 'E': 1}\r\n",
      "H\t{'B': 1, 'E': 1}\r\n",
      "I\t{'B': 1, 'E': 1}\r\n",
      "J\t{'E': 1}\r\n",
      "K\t{'E': 1}\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./PageRank-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapCount is for counting the true number of nodes\n",
    "# necessary since nodes with no outgoing links are not listed on their own line\n",
    "\n",
    "def mapCount(entry):\n",
    "    yield (str(entry[0]), 1)\n",
    "    for key, value in entry[1].items():\n",
    "        yield (key, 1)\n",
    "        \n",
    "# processLine turns the string into python objects,\n",
    "# specifically the label and a dictionary of edges.\n",
    "# this assumes zero formatting errors\n",
    "def processLine(line):\n",
    "    splits = line.strip().split('\\t')\n",
    "    return [splits[0], eval(splits[1])]\n",
    "\n",
    "# prep_map generates a line for each possible node, including nodes that exist\n",
    "# only in the edge dictionary of another node\n",
    "def prep_map(entry):\n",
    "    nodeID = entry[0]\n",
    "    edges = entry[1]\n",
    "    for edge, weight in edges.items():\n",
    "        yield (str(edge.strip()), {})\n",
    "    yield (str(nodeID), edges)\n",
    "    \n",
    "# prep_reduce combines the dictionaries of the records created in the map phase\n",
    "def prep_reduce(x, y):\n",
    "    edges = {}\n",
    "    for key, value in x.items():\n",
    "        edges[key] = value\n",
    "    for key, value in y.items():\n",
    "        edges[key] = value\n",
    "    return edges\n",
    "\n",
    "# init_entry distributes the initial probability mass\n",
    "# it also adds a value into the record of 0.0 which will eventually represent\n",
    "# the amount of change in that node's probability mass\n",
    "# which we'll use to check for convergence\n",
    "# it is reliant on the accum_total_pr accumulator for its closure\n",
    "\n",
    "accum_total_pr = sc.accumulator(0.0)\n",
    "\n",
    "def init_entry(entry):\n",
    "    accum_total_pr.add(1.0 / broadcast_nodecount.value)\n",
    "    return (entry[0], [entry[1], 1.0 / broadcast_nodecount.value, 0.0]) #the 0.0 is the delta pr \n",
    "\n",
    "\n",
    "# Phase one consists of a map and reducebykey phase.\n",
    "# phaseOneMapper stores the pr mass of the node as it is as the beginning of the step\n",
    "# if there are no outgoing edges, it passes all the pr mass of that node to the dangling_mass accumulator\n",
    "# which is defined here for the closure.\n",
    "# if there are edges, it divides the pr mass evenly among them and emits the target node with \n",
    "# an empty edge list and a zero previous pr mass.  This is done so that when records are combined\n",
    "# in the reduce step, there will be exactly one record with a full edge list and an accurate\n",
    "# previous pr mass, so we can cummutatively and associatively add the records together\n",
    "# and arrive at a single accurate record\n",
    "\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "\n",
    "\n",
    "def phaseOneMapper(entry):\n",
    "    label = entry[0]\n",
    "    edges = entry[1][0]\n",
    "    pr = entry[1][1]\n",
    "    previous = pr\n",
    "\n",
    "    if len(edges) == 0:\n",
    "        accum_dangling_mass.add(pr)\n",
    "    else:\n",
    "        forwarding_pr = pr / len(edges)\n",
    "        for edge, weight in edges.items():\n",
    "            yield (edge, [{}, forwarding_pr, 0.0])\n",
    "    yield (label, [edges, 0.0, pr])\n",
    "\n",
    "def phaseOneReducer(x, y):\n",
    "    edges = {}\n",
    "    for edge, weight in x[0].items():\n",
    "        edges[edge] = weight\n",
    "    for edge, weight in y[0].items():\n",
    "        edges[edge] = weight\n",
    "    return [edges, x[1] + y[1], x[2] + y[2]]\n",
    "\n",
    "# the finalize state distributes the dangling pr mass by knowing from the broadcast variables how many\n",
    "# nodes there are and by being passed the dangling mass from a variable defined with the accumulator\n",
    "# in the driver program logic.  It also calculates the final pr mass of the node by the pagerank equation\n",
    "# additionally, it records the absolute value of how much the pr mass of this node has changed and\n",
    "# passes it to the accum_moved_mass accumulator.  \n",
    "# Since the original pr mass value is no longer needed, it is not stored\n",
    "# (recall that it is regenerated in the phase one map step and is simply the value here called pr_prime)\n",
    "\n",
    "accum_moved_mass = sc.accumulator(0.0)\n",
    "\n",
    "def finalize(entry, dangling_mass):\n",
    "    label = str(entry[0])\n",
    "    edges = entry[1][0]\n",
    "    pr = entry[1][1]\n",
    "    previous = entry[1][2]\n",
    "    \n",
    "    pr_prime = broadcast_damping_factor.value * (1.0 / broadcast_nodecount.value) + \\\n",
    "    (1 - broadcast_damping_factor.value) * (dangling_mass / broadcast_nodecount.value + pr)\n",
    "    \n",
    "    accum_moved_mass.add(((pr_prime - previous)**2)**.5)\n",
    "    \n",
    "    return (label, [edges, pr_prime])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 nodes in the dataset\n",
      "(0.0969820022583)\n",
      "\n",
      "Data prepped\n",
      "(0.12645483017)\n",
      "\n",
      "Initial dangling mass: 0.0\n",
      "Dangling mass distributed\n",
      "(0.108367919922)\n",
      "\n",
      "Iteration 1 runtime: 0.207234144211\n",
      "Iteration 2 runtime: 0.215823173523\n",
      "Iteration 3 runtime: 0.212052106857\n",
      "Iteration 4 runtime: 0.201957941055\n",
      "Iteration 5 runtime: 0.217806100845\n",
      "Iteration 6 runtime: 0.205757856369\n",
      "Iteration 7 runtime: 0.214182853699\n",
      "Iteration 8 runtime: 0.208344221115\n",
      "Iteration 9 runtime: 0.419463157654\n",
      "Iteration 10 runtime: 0.231929063797\n",
      "Iteration 11 runtime: 0.20774102211\n",
      "Iteration 12 runtime: 0.21975684166\n",
      "Iteration 13 runtime: 0.224045991898\n",
      "Iteration 14 runtime: 0.236083030701\n",
      "Iteration 15 runtime: 0.227634906769\n",
      "Iteration 16 runtime: 0.210209846497\n",
      "Iteration 17 runtime: 0.213737010956\n",
      "Iteration 18 runtime: 0.211986064911\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "\n",
    "#load the data and process it\n",
    "data = sc.textFile(\"./PageRank-test.txt\").map(processLine)\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# count the true number of nodes\n",
    "start_time = time.time()\n",
    "nodecount = data.flatMap(mapCount).reduceByKey(lambda x, y: 1).count()\n",
    "nodecount_time = time.time() - start_time\n",
    "broadcast_nodecount = sc.broadcast(nodecount)\n",
    "\n",
    "print \"There are \" + str(broadcast_nodecount.value) + \" nodes in the dataset\"\n",
    "print \"(\" + str(nodecount_time) + \")\\n\"\n",
    "\n",
    "# conduct the initial preparations\n",
    "prepped_data = data.flatMap(prep_map).reduceByKey(prep_reduce).map(init_entry)\n",
    "\n",
    "broadcast_damping_factor = sc.broadcast(.15)\n",
    "\n",
    "accum_dangling_mass = sc.accumulator(0.0)\n",
    "accum_moved_mass = sc.accumulator(1.0)\n",
    "accum_total_pr = sc.accumulator(0.0)\n",
    "\n",
    "# conduct one initial pass\n",
    "start_time = time.time()\n",
    "prepped_data.flatMap(phaseOneMapper).reduceByKey(phaseOneReducer).cache()\n",
    "prepped_data.take(1)\n",
    "prep_time = time.time() - start_time\n",
    "print \"Data prepped\"\n",
    "print \"(\" + str(prep_time) + \")\\n\"\n",
    "\n",
    "dangling_mass = accum_dangling_mass.value\n",
    "\n",
    "print \"Initial dangling mass: \" + str(dangling_mass)\n",
    "\n",
    "start_time = time.time()\n",
    "intermediate_data = prepped_data.map(lambda x: finalize(x, dangling_mass)).cache()\n",
    "intermediate_data.take(1)\n",
    "distribute_time = time.time() - start_time\n",
    "print \"Dangling mass distributed\"\n",
    "print \"(\" + str(distribute_time) + \")\\n\"\n",
    "\n",
    "\n",
    "# loop until converged byt checking how much pr mass has moved.\n",
    "\n",
    "deltas = []\n",
    "i = 0\n",
    "while accum_moved_mass.value >= .001 and i < 40:\n",
    "    i += 1\n",
    "    if verbose:\n",
    "        print \"Iteration: \" + str(i)\n",
    "    accum_dangling_mass = sc.accumulator(0.0)\n",
    "    accum_moved_mass = sc.accumulator(0.0)\n",
    "    \n",
    "    iteration_start_time = time.time()\n",
    "\n",
    "    intermediate_data = intermediate_data.flatMap(phaseOneMapper).reduceByKey(phaseOneReducer)\n",
    "    intermediate_data.cache()\n",
    "    intermediate_data.take(1)\n",
    "    \n",
    "    \n",
    "    dangling_mass = accum_dangling_mass.value\n",
    "    if verbose:\n",
    "        print \"Total dangling mass: \" + str(dangling_mass)\n",
    "\n",
    "    intermediate_data = intermediate_data.map(lambda x: finalize(x, dangling_mass))\n",
    "    intermediate_data.cache()\n",
    "    intermediate_data.take(1)\n",
    "    if verbose:\n",
    "        print \"Moved mass: \" + str(accum_moved_mass.value) \n",
    "        print \"Total pr: \" + str(intermediate_data.map(lambda x: x[1][1]).reduce(lambda x, y: x + y)) + \"\\n\"\n",
    "    deltas.append(accum_moved_mass.value)\n",
    "    iteration_runtime = time.time() - iteration_start_time\n",
    "    print \"Iteration \" + str(i) + \" runtime: \" + str(iteration_runtime) \n",
    "    \n",
    "overall_runtime = time.time() - overall_start_time\n",
    "    \n",
    "#report results\n",
    "\n",
    "print \"\\n\\n\\n\"\n",
    "print \"Results:\\n\"\n",
    "print \"Iterations: \" + str(i)\n",
    "print \"Runtime: \" + str(overall_runtime)\n",
    "print \"Moved mass on final iteration: \" + str(accum_moved_mass.value)\n",
    "print \"Total pr :\" + str(intermediate_data.map(lambda x: x[1][1]).reduce(lambda x, y: x+y))\n",
    "plt.plot(deltas)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
