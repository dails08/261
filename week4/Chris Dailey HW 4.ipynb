{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "Chris Dailey\n",
    "\n",
    "christopher.dailey@gmail.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import re\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.0\n",
    "What is MrJob? How is it different to Hadoop MapReduce? \n",
    "What are the mappint_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.1\n",
    "What is serialization in the context of MrJob or Hadoop? \n",
    "When it used in these frameworks? \n",
    "What is the default serialization mode for input and outputs for MrJob? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.2\n",
    "\n",
    "Preprocess the Microsoft log data into a format that can be analyzed in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputFile = open(\"anonymous-msweb.data\", 'r')\n",
    "\n",
    "outputFile = open(\"cleanedlog.txt\", 'w')\n",
    "\n",
    "current_user = \"\"\n",
    "\n",
    "for line in inputFile.readlines():\n",
    "    if line[0] in [\"A\",\"T\",\"N\", \"I\"]:\n",
    "        outputFile.write(line)\n",
    "    elif line[0] == \"C\":\n",
    "        current_user = \"C,\" + re.split(',', line)[2].strip()\n",
    "    elif line[0] == \"V\":\n",
    "        outputFile.write(line.strip() + \",\" + current_user + \"\\n\")\n",
    "        \n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.3\n",
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pagefrequency.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pagefrequency.py\n",
    "import re\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.conf import combine_dicts\n",
    "\n",
    "class MRFrequencyUtility(MRJob):\n",
    "    \n",
    "    def step_two_conf(self):\n",
    "        #orig_jobconf = super(MRFrequencyUtility, self).jobconf()\n",
    "        new_jobconf= {\n",
    "            'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options':'-k2,2n'\n",
    "        }\n",
    "\n",
    "        #return combine_dicts(orig_jobconf, new_jobconf) \n",
    "        return new_jobconf\n",
    "    \n",
    "    def mapper_get_pages(self, _, line):\n",
    "        splits = [x.strip() for x in line.split(\",\")]\n",
    "        if splits[0] =='V':\n",
    "            yield (splits[1], 1)\n",
    "                    \n",
    "    def reducer_count_pages(self, page, counts):\n",
    "        yield  page, sum(counts)\n",
    "        \n",
    "    def mapper_sort_counts(self, page, counts):\n",
    "        yield count, page\n",
    "    \n",
    "    def reducer_sort_counts(self, page, counts):\n",
    "        for count in counts:\n",
    "            yield page, count\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_pages,\n",
    "                    reducer=self.reducer_count_pages),\n",
    "            MRStep(mapper=self.mapper_sort_counts,\n",
    "                   reducer=self.reducer_sort_counts,\n",
    "                   jobconf=self.step_two_conf())\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRFrequencyUtility.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/07 20:39:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/mrJobOutput/_SUCCESS\n",
      "16/02/07 20:39:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/mrJobOutput/part-00000\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/pagefrequency.ubuntu.20160207.203923.873940\n",
      "writing wrapper script to /tmp/pagefrequency.ubuntu.20160207.203923.873940/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/ubuntu/tmp/mrjob/pagefrequency.ubuntu.20160207.203923.873940/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar8296778798910507053/] [] /tmp/streamjob7358278926446340208.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1453815382601_0103\n",
      "HADOOP: Submitted application application_1453815382601_0103\n",
      "HADOOP: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0103/\n",
      "HADOOP: Running job: job_1453815382601_0103\n",
      "HADOOP: Job job_1453815382601_0103 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1453815382601_0103 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=1085200\n",
      "HADOOP: \t\tFILE: Number of bytes written=2531944\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=1692669\n",
      "HADOOP: \t\tHDFS: Number of bytes written=2903\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=10457\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=3424\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=10457\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=3424\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=10457\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=3424\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=10707968\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=3506176\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=98955\n",
      "HADOOP: \t\tMap output records=98654\n",
      "HADOOP: \t\tMap output bytes=887886\n",
      "HADOOP: \t\tMap output materialized bytes=1085206\n",
      "HADOOP: \t\tInput split bytes=320\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=285\n",
      "HADOOP: \t\tReduce shuffle bytes=1085206\n",
      "HADOOP: \t\tReduce input records=98654\n",
      "HADOOP: \t\tReduce output records=285\n",
      "HADOOP: \t\tSpilled Records=197308\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=178\n",
      "HADOOP: \t\tCPU time spent (ms)=3540\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=720629760\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=2497822720\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=560988160\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=1692349\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=2903\n",
      "HADOOP: Output directory: hdfs:///user/ubuntu/tmp/mrjob/pagefrequency.ubuntu.20160207.203923.873940/step-output/1\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar7982378650964135497/] [] /tmp/streamjob8688468065264677740.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "HADOOP: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "HADOOP: Submitting tokens for job: job_1453815382601_0104\n",
      "HADOOP: Submitted application application_1453815382601_0104\n",
      "HADOOP: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0104/\n",
      "HADOOP: Running job: job_1453815382601_0104\n",
      "HADOOP: Job job_1453815382601_0104 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1453815382601_0104 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=3479\n",
      "HADOOP: \t\tFILE: Number of bytes written=369393\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=4683\n",
      "HADOOP: \t\tHDFS: Number of bytes written=2903\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=8462\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=2680\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=8462\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=2680\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=8462\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=2680\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=8665088\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=2744320\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=285\n",
      "HADOOP: \t\tMap output records=285\n",
      "HADOOP: \t\tMap output bytes=2903\n",
      "HADOOP: \t\tMap output materialized bytes=3485\n",
      "HADOOP: \t\tInput split bytes=328\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=285\n",
      "HADOOP: \t\tReduce shuffle bytes=3485\n",
      "HADOOP: \t\tReduce input records=285\n",
      "HADOOP: \t\tReduce output records=285\n",
      "HADOOP: \t\tSpilled Records=570\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=128\n",
      "HADOOP: \t\tCPU time spent (ms)=1720\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=718143488\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=2504048640\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=560988160\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=4355\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=2903\n",
      "HADOOP: Output directory: hdfs:///user/ubuntu/mrJobOutput\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/ubuntu/mrJobOutput\n",
      "\"1207\"\t12\n",
      "\"1056\"\t276\n",
      "\"1191\"\t4\n",
      "\"1039\"\t345\n",
      "\"1239\"\t3\n",
      "\"1088\"\t237\n",
      "\"1215\"\t45\n",
      "\"1064\"\t324\n",
      "\"1175\"\t6\n",
      "\"1080\"\t121\n",
      "\"1247\"\t3\n",
      "\"1096\"\t214\n",
      "\"1199\"\t1\n",
      "\"1048\"\t210\n",
      "\"1263\"\t2\n",
      "\"1112\"\t128\n",
      "\"1183\"\t167\n",
      "\"1032\"\t1446\n",
      "\"1231\"\t19\n",
      "\"1108\"\t47\n",
      "\"1219\"\t4\n",
      "\"1068\"\t198\n",
      "\"1167\"\t72\n",
      "\"1076\"\t444\n",
      "\"1251\"\t11\n",
      "\"1100\"\t291\n",
      "\"1211\"\t16\n",
      "\"1060\"\t391\n",
      "\"1267\"\t5\n",
      "\"1116\"\t31\n",
      "\"1203\"\t55\n",
      "\"1052\"\t842\n",
      "\"1243\"\t4\n",
      "\"1092\"\t97\n",
      "\"1195\"\t15\n",
      "\"1043\"\t224\n",
      "\"1275\"\t1\n",
      "\"1124\"\t222\n",
      "\"1187\"\t38\n",
      "\"1035\"\t1791\n",
      "\"1235\"\t9\n",
      "\"1084\"\t186\n",
      "\"1179\"\t11\n",
      "\"1082\"\t241\n",
      "\"1259\"\t1\n",
      "\"1122\"\t13\n",
      "\"1171\"\t47\n",
      "\"1078\"\t462\n",
      "\"1227\"\t32\n",
      "\"1106\"\t16\n",
      "\"1221\"\t10\n",
      "\"1070\"\t602\n",
      "\"1225\"\t7\n",
      "\"1074\"\t584\n",
      "\"1253\"\t5\n",
      "\"1102\"\t118\n",
      "\"1217\"\t13\n",
      "\"1066\"\t82\n",
      "\"1269\"\t2\n",
      "\"1118\"\t172\n",
      "\"1213\"\t3\n",
      "\"1062\"\t141\n",
      "\"1249\"\t2\n",
      "\"1098\"\t98\n",
      "\"1209\"\t9\n",
      "\"1058\"\t672\n",
      "\"1277\"\t1\n",
      "\"1126\"\t27\n",
      "\"1205\"\t24\n",
      "\"1054\"\t338\n",
      "\"1245\"\t2\n",
      "\"1094\"\t14\n",
      "\"1201\"\t38\n",
      "\"1050\"\t106\n",
      "\"1265\"\t3\n",
      "\"1114\"\t48\n",
      "\"1197\"\t32\n",
      "\"1045\"\t474\n",
      "\"1241\"\t9\n",
      "\"1090\"\t107\n",
      "\"1193\"\t25\n",
      "\"1041\"\t1500\n",
      "\"1281\"\t1\n",
      "\"1130\"\t395\n",
      "\"1189\"\t51\n",
      "\"1037\"\t1160\n",
      "\"1237\"\t3\n",
      "\"1086\"\t22\n",
      "\"1185\"\t24\n",
      "\"1033\"\t26\n",
      "\"1261\"\t2\n",
      "\"1110\"\t46\n",
      "\"1181\"\t12\n",
      "\"1083\"\t105\n",
      "\"1233\"\t1\n",
      "\"1109\"\t59\n",
      "\"1177\"\t43\n",
      "\"1081\"\t215\n",
      "\"1273\"\t1\n",
      "\"1129\"\t7\n",
      "\"1173\"\t3\n",
      "\"1079\"\t136\n",
      "\"1229\"\t4\n",
      "\"1107\"\t11\n",
      "\"1169\"\t44\n",
      "\"1077\"\t155\n",
      "\"1257\"\t5\n",
      "\"1121\"\t63\n",
      "\"1226\"\t14\n",
      "\"1075\"\t396\n",
      "\"1256\"\t3\n",
      "\"1105\"\t183\n",
      "\"1222\"\t11\n",
      "\"1071\"\t187\n",
      "\"1224\"\t16\n",
      "\"1073\"\t204\n",
      "\"1254\"\t1\n",
      "\"1103\"\t36\n",
      "\"1220\"\t23\n",
      "\"1069\"\t227\n",
      "\"1270\"\t1\n",
      "\"1119\"\t365\n",
      "\"1218\"\t18\n",
      "\"1067\"\t548\n",
      "\"1252\"\t3\n",
      "\"1101\"\t14\n",
      "\"1216\"\t30\n",
      "\"1065\"\t323\n",
      "\"1278\"\t2\n",
      "\"1127\"\t132\n",
      "\"1214\"\t3\n",
      "\"1063\"\t113\n",
      "\"1250\"\t13\n",
      "\"1099\"\t120\n",
      "\"1212\"\t25\n",
      "\"1061\"\t269\n",
      "\"1268\"\t1\n",
      "\"1117\"\t9\n",
      "\"1210\"\t5\n",
      "\"1059\"\t258\n",
      "\"1248\"\t1\n",
      "\"1097\"\t56\n",
      "\"1208\"\t34\n",
      "\"1057\"\t195\n",
      "\"1282\"\t2\n",
      "\"1131\"\t148\n",
      "\"1206\"\t29\n",
      "\"1055\"\t264\n",
      "\"1246\"\t10\n",
      "\"1095\"\t102\n",
      "\"1204\"\t30\n",
      "\"1053\"\t670\n",
      "\"1266\"\t2\n",
      "\"1115\"\t15\n",
      "\"1202\"\t4\n",
      "\"1051\"\t86\n",
      "\"1244\"\t4\n",
      "\"1093\"\t65\n",
      "\"1200\"\t18\n",
      "\"1049\"\t343\n",
      "\"1276\"\t3\n",
      "\"1125\"\t199\n",
      "\"1198\"\t18\n",
      "\"1046\"\t636\n",
      "\"1242\"\t4\n",
      "\"1091\"\t69\n",
      "\"1196\"\t1\n",
      "\"1044\"\t168\n",
      "\"1264\"\t4\n",
      "\"1113\"\t181\n",
      "\"1194\"\t26\n",
      "\"1042\"\t281\n",
      "\"1240\"\t11\n",
      "\"1089\"\t157\n",
      "\"1192\"\t7\n",
      "\"1040\"\t1506\n",
      "\"1284\"\t1\n",
      "\"1133\"\t69\n",
      "\"1190\"\t48\n",
      "\"1038\"\t1110\n",
      "\"1238\"\t4\n",
      "\"1087\"\t189\n",
      "\"1188\"\t94\n",
      "\"1036\"\t759\n",
      "\"1262\"\t4\n",
      "\"1111\"\t36\n",
      "\"1186\"\t46\n",
      "\"1034\"\t9383\n",
      "\"1236\"\t10\n",
      "\"1085\"\t86\n",
      "\"1184\"\t57\n",
      "\"1123\"\t372\n",
      "\"1274\"\t1\n",
      "\"1031\"\t574\n",
      "\"1182\"\t7\n",
      "\"1132\"\t23\n",
      "\"1234\"\t8\n",
      "\"1128\"\t1\n",
      "\"1180\"\t9\n",
      "\"1120\"\t1\n",
      "\"1260\"\t1\n",
      "\"1104\"\t35\n",
      "\"1178\"\t2\n",
      "\"1072\"\t128\n",
      "\"1232\"\t4\n",
      "\"1030\"\t1115\n",
      "\"1176\"\t63\n",
      "\"1029\"\t132\n",
      "\"1280\"\t2\n",
      "\"1028\"\t93\n",
      "\"1174\"\t10\n",
      "\"1027\"\t507\n",
      "\"1230\"\t21\n",
      "\"1026\"\t3220\n",
      "\"1172\"\t45\n",
      "\"1025\"\t2123\n",
      "\"1258\"\t3\n",
      "\"1024\"\t521\n",
      "\"1170\"\t16\n",
      "\"1023\"\t191\n",
      "\"1228\"\t13\n",
      "\"1022\"\t325\n",
      "\"1168\"\t93\n",
      "\"1021\"\t380\n",
      "\"1272\"\t1\n",
      "\"1020\"\t1087\n",
      "\"1166\"\t33\n",
      "\"1019\"\t111\n",
      "\"1283\"\t1\n",
      "\"1018\"\t5330\n",
      "\"1279\"\t1\n",
      "\"1017\"\t5108\n",
      "\"1271\"\t1\n",
      "\"1016\"\t287\n",
      "\"1255\"\t3\n",
      "\"1015\"\t79\n",
      "\"1223\"\t29\n",
      "\"1014\"\t728\n",
      "\"1165\"\t38\n",
      "\"1013\"\t61\n",
      "\"1164\"\t49\n",
      "\"1012\"\t44\n",
      "\"1163\"\t25\n",
      "\"1011\"\t179\n",
      "\"1162\"\t48\n",
      "\"1010\"\t698\n",
      "\"1161\"\t16\n",
      "\"1009\"\t4628\n",
      "\"1160\"\t36\n",
      "\"1008\"\t10836\n",
      "\"1159\"\t41\n",
      "\"1007\"\t865\n",
      "\"1158\"\t90\n",
      "\"1006\"\t135\n",
      "\"1157\"\t124\n",
      "\"1005\"\t42\n",
      "\"1156\"\t75\n",
      "\"1004\"\t8463\n",
      "\"1155\"\t52\n",
      "\"1003\"\t2968\n",
      "\"1154\"\t67\n",
      "\"1002\"\t749\n",
      "\"1153\"\t8\n",
      "\"1001\"\t4451\n",
      "\"1152\"\t52\n",
      "\"1000\"\t912\n",
      "\"1151\"\t21\n",
      "\"1134\"\t162\n",
      "\"1150\"\t93\n",
      "\"1149\"\t12\n",
      "\"1148\"\t96\n",
      "\"1147\"\t86\n",
      "\"1146\"\t79\n",
      "\"1145\"\t20\n",
      "\"1144\"\t36\n",
      "\"1143\"\t60\n",
      "\"1142\"\t19\n",
      "\"1141\"\t36\n",
      "\"1140\"\t118\n",
      "\"1139\"\t18\n",
      "\"1138\"\t33\n",
      "\"1137\"\t123\n",
      "\"1136\"\t181\n",
      "\"1135\"\t115\n",
      "\"1295\"\t716\n",
      "removing tmp directory /tmp/pagefrequency.ubuntu.20160207.203923.873940\n",
      "deleting hdfs:///user/ubuntu/tmp/mrjob/pagefrequency.ubuntu.20160207.203923.873940 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/ubuntu/mrJobOutput/*\n",
    "!hdfs dfs -rmdir /user/ubuntu/mrJobOutput\n",
    "!python pagefrequency.py --jobconf -numReduceTasks=1 cleanedlog.txt --output-dir mrJobOutput -r hadoop \\\n",
    "--hadoop-home /home/ubuntu/hadoop-2.7.1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-02-07 20:24 /user/ubuntu/mrJobOutput/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-02-07 20:24 /user/ubuntu/mrJobOutput/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/ubuntu/mrJobOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/07 20:26:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/mrJobOutput/_SUCCESS\n",
      "16/02/07 20:26:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/mrJobOutput/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/ubuntu/mrJobOutput/*\n",
    "!hdfs dfs -rmdir /user/ubuntu/mrJobOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
