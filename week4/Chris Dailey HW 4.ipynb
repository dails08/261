{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "Chris Dailey\n",
    "\n",
    "christopher.dailey@gmail.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import re\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.0\n",
    "What is MrJob? How is it different to Hadoop MapReduce? \n",
    "What are the mappint_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.1\n",
    "What is serialization in the context of MrJob or Hadoop? \n",
    "When it used in these frameworks? \n",
    "What is the default serialization mode for input and outputs for MrJob? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.2\n",
    "\n",
    "Preprocess the Microsoft log data into a format that can be analyzed in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputFile = open(\"anonymous-msweb.data\", 'r')\n",
    "\n",
    "outputFile = open(\"cleanedlog.txt\", 'w')\n",
    "\n",
    "current_user = \"\"\n",
    "\n",
    "for line in inputFile.readlines():\n",
    "    if line[0] in [\"A\",\"T\",\"N\", \"I\"]:\n",
    "        outputFile.write(line)\n",
    "    elif line[0] == \"C\":\n",
    "        current_user = \"C,\" + re.split(',', line)[2].strip()\n",
    "    elif line[0] == \"V\":\n",
    "        outputFile.write(line.strip() + \",\" + current_user + \"\\n\")\n",
    "        \n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.3\n",
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pagefrequency.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pagefrequency.py\n",
    "import re\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.conf import combine_dicts\n",
    "\n",
    "class MRFrequencyUtility(MRJob):\n",
    "    \n",
    "    def step_two_conf(self):\n",
    "        #orig_jobconf = super(MRFrequencyUtility, self).jobconf()\n",
    "        new_jobconf= {\n",
    "            'stream.num.map.output.key.fields' : 2,\n",
    "            'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options':'-k2,2nr'\n",
    "        }\n",
    "\n",
    "        #return combine_dicts(orig_jobconf, new_jobconf) \n",
    "        return new_jobconf\n",
    "    \n",
    "    def mapper_get_pages(self, _, line):\n",
    "        splits = [x.strip() for x in line.split(\",\")]\n",
    "        if splits[0] =='V':\n",
    "            yield (splits[1], 1)\n",
    "                    \n",
    "    def reducer_count_pages(self, page, counts):\n",
    "        yield  page, sum(counts)\n",
    "        \n",
    "    def mapper_sort_counts(self, page, counts):\n",
    "        yield page, counts\n",
    "    \n",
    "    def reducer_sort_counts(self, page, counts):\n",
    "        for count in counts:\n",
    "            yield page, count\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_pages,\n",
    "                    reducer=self.reducer_count_pages),\n",
    "            MRStep(mapper=self.mapper_sort_counts,\n",
    "                   reducer=self.reducer_sort_counts,\n",
    "                   jobconf=self.step_two_conf())\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRFrequencyUtility.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/11 03:24:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/mrJobOutput/_SUCCESS\n",
      "16/02/11 03:24:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/mrJobOutput/part-00000\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/pagefrequency.ubuntu.20160211.032440.500578\n",
      "writing wrapper script to /tmp/pagefrequency.ubuntu.20160211.032440.500578/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/ubuntu/tmp/mrjob/pagefrequency.ubuntu.20160211.032440.500578/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar7970268806208798296/] [] /tmp/streamjob7295012397195802592.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1453815382601_0119\n",
      "HADOOP: Submitted application application_1453815382601_0119\n",
      "HADOOP: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0119/\n",
      "HADOOP: Running job: job_1453815382601_0119\n",
      "HADOOP: Job job_1453815382601_0119 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 50% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1453815382601_0119 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=1085200\n",
      "HADOOP: \t\tFILE: Number of bytes written=2531944\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=1692669\n",
      "HADOOP: \t\tHDFS: Number of bytes written=2903\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=10384\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=3456\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=10384\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=3456\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=10384\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=3456\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=10633216\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=3538944\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=98955\n",
      "HADOOP: \t\tMap output records=98654\n",
      "HADOOP: \t\tMap output bytes=887886\n",
      "HADOOP: \t\tMap output materialized bytes=1085206\n",
      "HADOOP: \t\tInput split bytes=320\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=285\n",
      "HADOOP: \t\tReduce shuffle bytes=1085206\n",
      "HADOOP: \t\tReduce input records=98654\n",
      "HADOOP: \t\tReduce output records=285\n",
      "HADOOP: \t\tSpilled Records=197308\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=126\n",
      "HADOOP: \t\tCPU time spent (ms)=3670\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=723935232\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=2490630144\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=560988160\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=1692349\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=2903\n",
      "HADOOP: Output directory: hdfs:///user/ubuntu/tmp/mrjob/pagefrequency.ubuntu.20160211.032440.500578/step-output/1\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar4265787549069299214/] [] /tmp/streamjob8736880201675050837.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "HADOOP: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "HADOOP: Submitting tokens for job: job_1453815382601_0120\n",
      "HADOOP: Submitted application application_1453815382601_0120\n",
      "HADOOP: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0120/\n",
      "HADOOP: Running job: job_1453815382601_0120\n",
      "HADOOP: Job job_1453815382601_0120 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1453815382601_0120 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=3764\n",
      "HADOOP: \t\tFILE: Number of bytes written=370389\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=4683\n",
      "HADOOP: \t\tHDFS: Number of bytes written=2903\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=8729\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=2737\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=8729\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=2737\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=8729\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=2737\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=8938496\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=2802688\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=285\n",
      "HADOOP: \t\tMap output records=285\n",
      "HADOOP: \t\tMap output bytes=3188\n",
      "HADOOP: \t\tMap output materialized bytes=3770\n",
      "HADOOP: \t\tInput split bytes=328\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=285\n",
      "HADOOP: \t\tReduce shuffle bytes=3770\n",
      "HADOOP: \t\tReduce input records=285\n",
      "HADOOP: \t\tReduce output records=285\n",
      "HADOOP: \t\tSpilled Records=570\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=99\n",
      "HADOOP: \t\tCPU time spent (ms)=1770\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=717975552\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=2492063744\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=560988160\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=4355\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=2903\n",
      "HADOOP: Output directory: hdfs:///user/ubuntu/mrJobOutput\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "removing tmp directory /tmp/pagefrequency.ubuntu.20160211.032440.500578\n",
      "deleting hdfs:///user/ubuntu/tmp/mrjob/pagefrequency.ubuntu.20160211.032440.500578 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/ubuntu/mrJobOutput/*\n",
    "!hdfs dfs -rmdir /user/ubuntu/mrJobOutput\n",
    "!python pagefrequency.py --jobconf -numReduceTasks=1 cleanedlog.txt --output-dir mrJobOutput -r hadoop \\\n",
    "--hadoop-home /home/ubuntu/hadoop-2.7.1/ --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-02-07 20:24 /user/ubuntu/mrJobOutput/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-02-07 20:24 /user/ubuntu/mrJobOutput/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/ubuntu/mrJobOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1008\"\t10836\r\n",
      "\"1034\"\t9383\r\n",
      "\"1004\"\t8463\r\n",
      "\"1018\"\t5330\r\n",
      "\"1017\"\t5108\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/ubuntu/mrJobOutput/part-00000 | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.4\n",
    "Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting maxvisitors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile maxvisitors.py\n",
    "import re\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.conf import combine_dicts\n",
    "\n",
    "class MRMaxVisitors(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def mapper_get_page_visits(self, _, line):\n",
    "        splits = [x.strip() for x in line.split(\",\")]\n",
    "        if splits[0] =='V':\n",
    "            yield ((splits[1], splits[4]), 1)\n",
    "                    \n",
    "    def reducer_count_page_visits(self, pair, counts):\n",
    "        yield  pair, sum(counts)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_page_visits,\n",
    "                   reducer=self.reducer_count_page_visits)\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRMaxVisitors.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/11 03:26:05 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/mrJobOutput/_SUCCESS\n",
      "16/02/11 03:26:05 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/mrJobOutput/part-00000\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/maxvisitors.ubuntu.20160211.032608.473911\n",
      "writing wrapper script to /tmp/maxvisitors.ubuntu.20160211.032608.473911/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/ubuntu/tmp/mrjob/maxvisitors.ubuntu.20160211.032608.473911/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar6375874665555275655/] [] /tmp/streamjob1927153640174104626.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "HADOOP: Submitting tokens for job: job_1453815382601_0121\n",
      "HADOOP: Submitted application application_1453815382601_0121\n",
      "HADOOP: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0121/\n",
      "HADOOP: Running job: job_1453815382601_0121\n",
      "HADOOP: Job job_1453815382601_0121 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1453815382601_0121 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=2269048\n",
      "HADOOP: \t\tFILE: Number of bytes written=4900834\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=1692665\n",
      "HADOOP: \t\tHDFS: Number of bytes written=1973080\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=11106\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=4460\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=11106\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=4460\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=11106\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=4460\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=11372544\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=4567040\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=98955\n",
      "HADOOP: \t\tMap output records=98654\n",
      "HADOOP: \t\tMap output bytes=2071734\n",
      "HADOOP: \t\tMap output materialized bytes=2269054\n",
      "HADOOP: \t\tInput split bytes=316\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=98654\n",
      "HADOOP: \t\tReduce shuffle bytes=2269054\n",
      "HADOOP: \t\tReduce input records=98654\n",
      "HADOOP: \t\tReduce output records=98654\n",
      "HADOOP: \t\tSpilled Records=197308\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=148\n",
      "HADOOP: \t\tCPU time spent (ms)=4350\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=712577024\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=2492489728\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=560988160\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=1692349\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=1973080\n",
      "HADOOP: Output directory: hdfs:///user/ubuntu/mrJobOutput\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "removing tmp directory /tmp/maxvisitors.ubuntu.20160211.032608.473911\n",
      "deleting hdfs:///user/ubuntu/tmp/mrjob/maxvisitors.ubuntu.20160211.032608.473911 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/ubuntu/mrJobOutput/*\n",
    "!hdfs dfs -rmdir /user/ubuntu/mrJobOutput\n",
    "!python maxvisitors.py --jobconf -numReduceTasks=1 cleanedlog.txt --output-dir mrJobOutput -r hadoop \\\n",
    "--hadoop-home /home/ubuntu/hadoop-2.7.1/ --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1000\", \"10001\"]\t1\r\n",
      "[\"1000\", \"10010\"]\t1\r\n",
      "[\"1000\", \"10039\"]\t1\r\n",
      "[\"1000\", \"10073\"]\t1\r\n",
      "[\"1000\", \"10087\"]\t1\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/ubuntu/mrJobOutput/part-00000 | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.5\n",
    "n-dimensional k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 4.5A\n",
    "K=4 uniform random centroid-distributions over the 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    #print \"Nearest: \" + str(minidx)\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    return Diff\n",
    "#     Flag = True\n",
    "#     for i in Diff:\n",
    "#         if(i>T):\n",
    "#             Flag = False\n",
    "#             break\n",
    "#     return Flag\n",
    "\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=4    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init,\n",
    "                      mapper=self.mapper,\n",
    "                      combiner = self.combiner,\n",
    "                      reducer=self.reducer)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        open('Centroids.txt', 'w').close()\n",
    "    \n",
    "    \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        #yield int(MinDist(D,self.centroid_points)), (D[0],D[1],1)\n",
    "        yield int(MinDist(D[3:],self.centroid_points)), (D[3:],1)\n",
    "    \n",
    "    \n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        #sumx = sumy = num = 0\n",
    "        sums = [0]*1000\n",
    "        num = 0\n",
    "        for data ,n in inputdata:\n",
    "            num = num + n\n",
    "            sums = [x+y for x,y in zip(sums, data)]\n",
    "        yield idx,(sums,num)\n",
    "\n",
    "        \n",
    "        #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        num = [0]*self.k \n",
    "        for i in range(self.k):\n",
    "            centroids.append([0]*1000)\n",
    "        for data, n in inputdata:\n",
    "            num[idx] = num[idx] + n\n",
    "            for i in range(len(data)):\n",
    "                centroids[idx][i] = centroids[idx][i] + data[i]\n",
    "        for i in range(1000):\n",
    "            centroids[idx][i] = centroids[idx][i]/num[idx]\n",
    "        with open('Centroids.txt', 'a') as f:\n",
    "            for centroid in centroids:\n",
    "                if sum(centroid) > 0:\n",
    "                    f.writelines(','.join(str(j) for j in centroid) + '\\n')\n",
    "#            centroid_string = \"\"\n",
    "#            for i in range(1000):\n",
    "#                centroid_string += str(inputdata[0][i])\n",
    "#                if i < 999:\n",
    "#                    centroid_string += \", \"\n",
    "#            f.writelines(centroid_string + '\\n')\n",
    "#            f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroids)\n",
    "\n",
    "\n",
    "        yield idx,(centroids[idx])\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Diff: 360260.229493\n",
      "iteration1:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Diff: 329919.552024\n",
      "iteration2:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Diff: 480202.803201\n",
      "iteration3:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Diff: 185327.80206\n",
      "iteration4:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Diff: 277292.168453\n",
      "iteration5:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Diff: 21165.1685662\n",
      "iteration6:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Diff: 33458.9513205\n",
      "iteration7:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Diff: 28492.3067543\n",
      "iteration8:\n",
      "\n",
      "\n",
      "\n",
      "Sum of Diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from numpy import random\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "import warnings\n",
    "import re, random as rnd\n",
    "mr_job = MRKmeans(args=[\"topUsers_Apr-Jul_2014_1000-words.txt\", \"--file\", \"Centroids.txt\"])\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Geneate initial centroids\n",
    "centroid_points = []\n",
    "k = 4\n",
    "\n",
    "word_data = open(\"topUsers_Apr-Jul_2014_1000-words.txt\", 'r')\n",
    "\n",
    "data_array = []\n",
    "\n",
    "#this works with a small sample size but could be done just as well with any random sample\n",
    "#of a larger dataset\n",
    "for line in word_data.readlines():\n",
    "    data_array.append([int(x.strip()) for x in re.split(',', line)[3:]])\n",
    "    \n",
    "word_data.close()\n",
    "    \n",
    "for i in range(k):\n",
    "    centroid_points.append(data_array[int(rnd.random()*len(data_array))])\n",
    "\n",
    "with open('Centroids.txt', 'w') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "        \n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i)+\":\\n\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        with warnings.catch_warnings():\n",
    "            runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            centroid_points[key] = value\n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "    Diff = stop_criterion(centroid_points_old, centroid_points, 0.01)\n",
    "#    if(stop_criterion(centroid_points_old,centroid_points,0.01)):\n",
    "    done = True\n",
    "    for m in Diff:\n",
    "        if(m>0.01):\n",
    "            done = False\n",
    "    print \"Sum of Diff: \" + str(sum(Diff))\n",
    "    if done:\n",
    "        break\n",
    "#print \"Centroids\\n\"\n",
    "#print centroid_points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
