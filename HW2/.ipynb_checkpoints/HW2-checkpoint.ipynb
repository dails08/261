{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 2\n",
    "\n",
    "Chris Dailey\n",
    "\n",
    "christopher.dailey@gmail.com\n",
    "\n",
    "W261-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial HDFS setup and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.txt\n",
    "hello hi hi hallo\n",
    "bonjour hola hi ciao\n",
    "nihao konnichiwa ola\n",
    "hola nihao hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print(word+\", 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, 1\r\n",
      "hi, 1\r\n",
      "hi, 1\r\n",
      "hallo, 1\r\n",
      "bonjour, 1\r\n",
      "hola, 1\r\n",
      "hi, 1\r\n",
      "ciao, 1\r\n",
      "nihao, 1\r\n",
      "konnichiwa, 1\r\n",
      "ola, 1\r\n",
      "hola, 1\r\n",
      "nihao, 1\r\n",
      "hello, 1\r\n"
     ]
    }
   ],
   "source": [
    "!cat wordcount.txt | python mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split(\",\")\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print(current_word + \"\\t\" + str(current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print(current_word + \"\\t\" + str(current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\t1\r\n",
      "foo\t3\r\n",
      "labs\t1\r\n",
      "quux\t2\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | python mapper.py | sort -k1,1 | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/ubuntu/wordcountOutput/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -rm /user/ubuntu/wordcountOutput/*\n",
    "!hadoop/bin/hdfs dfs -rmdir /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: hdfs: not found\r\n"
     ]
    }
   ],
   "source": [
    "hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -file ~/repos/261/HW2/mapper.py -mapper ~/repos/261/HW2/mapper.py -file ~/repos/261/HW2/reducer.py -reducer ~/repos/261/HW2/reducer.py -input /user/ubuntu/wordcount.txt -output /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonjour\t1\r\n",
      "ciao\t1\r\n",
      "hallo\t1\r\n",
      "hello\t2\r\n",
      "hi\t3\r\n",
      "hola\t2\r\n",
      "konnichiwa\t1\r\n",
      "nihao\t2\r\n",
      "ola\t1\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar3869171142529360995/] [] /tmp/streamjob4695714299996201914.jar tmpDir=null\n",
      "16/01/24 18:10:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 18:10:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 18:10:19 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 18:10:19 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/24 18:10:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453569821305_0011\n",
      "16/01/24 18:10:19 INFO impl.YarnClientImpl: Submitted application application_1453569821305_0011\n",
      "16/01/24 18:10:19 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453569821305_0011/\n",
      "16/01/24 18:10:19 INFO mapreduce.Job: Running job: job_1453569821305_0011\n",
      "16/01/24 18:10:24 INFO mapreduce.Job: Job job_1453569821305_0011 running in uber mode : false\n",
      "16/01/24 18:10:24 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 18:10:31 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 18:10:36 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 18:10:36 INFO mapreduce.Job: Job job_1453569821305_0011 completed successfully\n",
      "16/01/24 18:10:37 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=167\n",
      "\t\tFILE: Number of bytes written=352404\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=312\n",
      "\t\tHDFS: Number of bytes written=72\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8362\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2702\n",
      "\t\tTotal time spent by all map tasks (ms)=8362\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2702\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8362\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2702\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8562688\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2766848\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=14\n",
      "\t\tMap output bytes=133\n",
      "\t\tMap output materialized bytes=173\n",
      "\t\tInput split bytes=198\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce shuffle bytes=173\n",
      "\t\tReduce input records=14\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=28\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=131\n",
      "\t\tCPU time spent (ms)=1580\n",
      "\t\tPhysical memory (bytes) snapshot=721784832\n",
      "\t\tVirtual memory (bytes) snapshot=2495213568\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=114\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=72\n",
      "16/01/24 18:10:37 INFO streaming.StreamJob: Output directory: /user/ubuntu/wordcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper ~/repos/261/HW2/mapper.py -reducer ~/repos/261/HW2/reducer.py -input /user/ubuntu/wordcount.txt -output /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonjour\t1\r\n",
      "ciao\t1\r\n",
      "hallo\t1\r\n",
      "hello\t2\r\n",
      "hi\t3\r\n",
      "hola\t2\r\n",
      "konnichiwa\t1\r\n",
      "nihao\t2\r\n",
      "ola\t1\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.1\n",
    "Sort in Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hadoop/bin/hdfs dfs -mkdir /user/ubuntu/sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import random as rnd\n",
    "\n",
    "integers = open(\"integers.txt\", 'w')\n",
    "\n",
    "for i in range(10000):\n",
    "    integers.write(str(int(rnd.random()*5000))+\", \\n\")\n",
    "integers.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    integer, blank = line.split(\",\")\n",
    "    print(str(integer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "integers = []\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    integers.append(int(line))\n",
    "\n",
    "print(\"Ten biggest:\")\n",
    "for i in range(10):\n",
    "    print(integers[i])\n",
    "print(\"\\n\\n\")\n",
    "print(\"Ten smallest:\")\n",
    "for i in range(len(integers)-10, len(integers)):\n",
    "    print(integers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar7102658785291136101/] [] /tmp/streamjob5701401045672634559.jar tmpDir=null\n",
      "16/01/24 21:11:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 21:11:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 21:11:33 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 21:11:33 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/24 21:11:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453569821305_0018\n",
      "16/01/24 21:11:34 INFO impl.YarnClientImpl: Submitted application application_1453569821305_0018\n",
      "16/01/24 21:11:34 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453569821305_0018/\n",
      "16/01/24 21:11:34 INFO mapreduce.Job: Running job: job_1453569821305_0018\n",
      "16/01/24 21:11:39 INFO mapreduce.Job: Job job_1453569821305_0018 running in uber mode : false\n",
      "16/01/24 21:11:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 21:11:46 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 21:11:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 21:11:51 INFO mapreduce.Job: Job job_1453569821305_0018 completed successfully\n",
      "16/01/24 21:11:51 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=77750\n",
      "\t\tFILE: Number of bytes written=508647\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=70942\n",
      "\t\tHDFS: Number of bytes written=125\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9485\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2960\n",
      "\t\tTotal time spent by all map tasks (ms)=9485\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2960\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9485\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2960\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9712640\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3031040\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=57744\n",
      "\t\tMap output materialized bytes=77756\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4299\n",
      "\t\tReduce shuffle bytes=77756\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=25\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=101\n",
      "\t\tCPU time spent (ms)=2860\n",
      "\t\tPhysical memory (bytes) snapshot=713629696\n",
      "\t\tVirtual memory (bytes) snapshot=2500771840\n",
      "\t\tTotal committed heap usage (bytes)=534773760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=70736\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=125\n",
      "16/01/24 21:11:51 INFO streaming.StreamJob: Output directory: /user/ubuntu/sort/output\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=-k1nr \\\n",
    "-mapper /home/ubuntu/repos/261/HW2/mapper.py \\\n",
    "-reducer /home/ubuntu/repos/261/HW2/reducer.py \\\n",
    "-input /user/ubuntu/sort/integers.txt \\\n",
    "-output /user/ubuntu/sort/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-01-24 21:11 /user/ubuntu/sort/output/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu supergroup        125 2016-01-24 21:11 /user/ubuntu/sort/output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -ls /user/ubuntu/sort/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten biggest:\t\r\n",
      "4998\t\r\n",
      "4998\t\r\n",
      "4998\t\r\n",
      "4997\t\r\n",
      "4996\t\r\n",
      "4996\t\r\n",
      "4996\t\r\n",
      "4995\t\r\n",
      "4995\t\r\n",
      "4995\t\r\n",
      "\t\r\n",
      "\t\r\n",
      "\t\r\n",
      "Ten smallest:\t\r\n",
      "4\t\r\n",
      "4\t\r\n",
      "4\t\r\n",
      "3\t\r\n",
      "3\t\r\n",
      "2\t\r\n",
      "2\t\r\n",
      "1\t\r\n",
      "1\t\r\n",
      "0\t\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/sort/output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 21:11:26 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/sort/output/_SUCCESS\n",
      "16/01/24 21:11:26 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ubuntu/sort/output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -rm /user/ubuntu/sort/output/*\n",
    "!hadoop/bin/hdfs dfs -rmdir /user/ubuntu/sort/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.2\n",
    "\n",
    "Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re\n",
    "# input comes from STDIN (standard input)\n",
    "findword = sys.argv[1]\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    for word in [x.lower() for x in WORD_RE.findall(line)]:\n",
    "        if word == findword:\n",
    "            print(word+\", 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "currentWord= None\n",
    "currentCount = 0\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split(\",\")\n",
    "    if currentWord is None:\n",
    "        currentWord = word.lower()\n",
    "    if word.lower() == currentWord:\n",
    "        currentCount += int(count.rstrip())\n",
    "    else:\n",
    "        print(currentWord + \" \" + str(currentCount))\n",
    "        currentWord = word\n",
    "        currentCount = 0\n",
    "print(currentWord + \" \" + str(currentCount))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar4096940200557490376/] [] /tmp/streamjob6170588845171672975.jar tmpDir=null\n",
      "16/01/24 22:33:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 22:33:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 22:33:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 22:33:50 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/24 22:33:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453569821305_0022\n",
      "16/01/24 22:33:50 INFO impl.YarnClientImpl: Submitted application application_1453569821305_0022\n",
      "16/01/24 22:33:50 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453569821305_0022/\n",
      "16/01/24 22:33:50 INFO mapreduce.Job: Running job: job_1453569821305_0022\n",
      "16/01/24 22:33:55 INFO mapreduce.Job: Job job_1453569821305_0022 running in uber mode : false\n",
      "16/01/24 22:33:55 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 22:34:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 22:34:07 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 22:34:07 INFO mapreduce.Job: Job job_1453569821305_0022 completed successfully\n",
      "16/01/24 22:34:08 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=176\n",
      "\t\tFILE: Number of bytes written=352497\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216905\n",
      "\t\tHDFS: Number of bytes written=15\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8712\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2777\n",
      "\t\tTotal time spent by all map tasks (ms)=8712\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2777\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8712\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2777\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8921088\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2843648\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=150\n",
      "\t\tMap output materialized bytes=182\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=182\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=132\n",
      "\t\tCPU time spent (ms)=1670\n",
      "\t\tPhysical memory (bytes) snapshot=744861696\n",
      "\t\tVirtual memory (bytes) snapshot=2517192704\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=15\n",
      "16/01/24 22:34:08 INFO streaming.StreamJob: Output directory: /user/ubuntu/wordcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper \"/home/ubuntu/repos/261/HW2/mapper.py assistance\" \\\n",
    "-reducer /home/ubuntu/repos/261/HW2/reducer.py \\\n",
    "-input /user/ubuntu/wordcount/enronemail_1h.txt \\\n",
    "-output /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-01-24 22:34 /user/ubuntu/wordcountOutput/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu supergroup         15 2016-01-24 22:34 /user/ubuntu/wordcountOutput/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -ls /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance 10\t\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/ubuntu/wordcountOutput/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -rm /user/ubuntu/wordcountOutput/*\n",
    "!hadoop/bin/hdfs dfs -rmdir /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.2.1\n",
    "\n",
    "Finding the top ten most common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re\n",
    "# input comes from STDIN (standard input)\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    for word in [x.lower() for x in WORD_RE.findall(line)]:\n",
    "        print(word+\", 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "counts = {}\n",
    "\n",
    "currentWord= None\n",
    "currentCount = 0\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split(\",\")\n",
    "    if currentWord is None:\n",
    "        currentWord = word.lower()\n",
    "    if word.lower() == currentWord:\n",
    "        currentCount += int(count.rstrip())\n",
    "    else:\n",
    "        counts[currentWord] = currentCount\n",
    "        currentWord = word\n",
    "        currentCount = int(count.rstrip())\n",
    "counts[currentWord] = currentCount\n",
    "\n",
    "\n",
    "print(\"Top ten most common tokens:\")\n",
    "for key in sorted(counts, reverse=True)[:10]:\n",
    "    print(key + \" \" + str(counts[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.txt\n",
    "cat, 10\n",
    "cat, 5\n",
    "hat, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten most common tokens:\r\n",
      "cat 15\r\n",
      "hat 6\r\n"
     ]
    }
   ],
   "source": [
    "!cat test.txt | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/ubuntu/wordcountOutput/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -rm /user/ubuntu/wordcountOutput/*\n",
    "!hadoop/bin/hdfs dfs -rmdir /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar2650147028771454319/] [] /tmp/streamjob5395162985563137689.jar tmpDir=null\n",
      "16/01/24 23:16:41 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 23:16:41 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/24 23:16:41 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 23:16:41 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/24 23:16:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453569821305_0024\n",
      "16/01/24 23:16:42 INFO impl.YarnClientImpl: Submitted application application_1453569821305_0024\n",
      "16/01/24 23:16:42 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453569821305_0024/\n",
      "16/01/24 23:16:42 INFO mapreduce.Job: Running job: job_1453569821305_0024\n",
      "16/01/24 23:16:47 INFO mapreduce.Job: Job job_1453569821305_0024 running in uber mode : false\n",
      "16/01/24 23:16:47 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 23:16:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 23:16:59 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 23:16:59 INFO mapreduce.Job: Job job_1453569821305_0024 completed successfully\n",
      "16/01/24 23:16:59 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=389923\n",
      "\t\tFILE: Number of bytes written=1131958\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216905\n",
      "\t\tHDFS: Number of bytes written=131\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9319\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3048\n",
      "\t\tTotal time spent by all map tasks (ms)=9319\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3048\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9319\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3048\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9542656\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3121152\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=33511\n",
      "\t\tMap output bytes=322895\n",
      "\t\tMap output materialized bytes=389929\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5511\n",
      "\t\tReduce shuffle bytes=389929\n",
      "\t\tReduce input records=33511\n",
      "\t\tReduce output records=11\n",
      "\t\tSpilled Records=67022\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=82\n",
      "\t\tCPU time spent (ms)=3200\n",
      "\t\tPhysical memory (bytes) snapshot=723623936\n",
      "\t\tVirtual memory (bytes) snapshot=2492473344\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=131\n",
      "16/01/24 23:16:59 INFO streaming.StreamJob: Output directory: /user/ubuntu/wordcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/ubuntu/repos/261/HW2/mapper.py \\\n",
    "-reducer /home/ubuntu/repos/261/HW2/reducer.py \\\n",
    "-input /user/ubuntu/wordcount/enronemail_1h.txt \\\n",
    "-output /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-01-24 23:16 /user/ubuntu/wordcountOutput/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu supergroup        131 2016-01-24 23:16 /user/ubuntu/wordcountOutput/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -ls /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten most common tokens:\t\r\n",
      "' 3\t\r\n",
      "'bcli 1\t\r\n",
      "'benewm 1\t\r\n",
      "'bjeffrie 1\t\r\n",
      "'blong 1\t\r\n",
      "'bredd 1\t\r\n",
      "'celias 1\t\r\n",
      "'cenochs 1\t\r\n",
      "'chuck 1\t\r\n",
      "'colliw 1\t\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
