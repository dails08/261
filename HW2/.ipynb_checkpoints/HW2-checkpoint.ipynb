{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 2\n",
    "\n",
    "Chris Dailey\n",
    "\n",
    "christopher.dailey@gmail.com\n",
    "\n",
    "W261-4\n",
    "\n",
    "********NOTE***********\n",
    "All jobs are done in two stages: training and applying.  The applying stage a a map-only job that reads in the priors and condprobs and reads the emails from hdfs.  There are multiple mappers so accuracy is reported for each mapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.1\n",
    "Sort in Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/ubuntu/sort': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -mkdir /user/ubuntu/sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import random as rnd\n",
    "\n",
    "integers = open(\"integers.txt\", 'w')\n",
    "\n",
    "for i in range(10000):\n",
    "    integers.write(str(int(rnd.random()*5000))+\", \\n\")\n",
    "integers.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/ubuntu/sort/integers.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put integers.txt /user/ubuntu/sort/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    integer, blank = line.split(\",\")\n",
    "    print(str(integer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "integers = []\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    integers.append(int(line))\n",
    "\n",
    "print(\"Ten biggest:\")\n",
    "for i in range(10):\n",
    "    print(integers[i])\n",
    "print(\"\\n\\n\")\n",
    "print(\"Ten smallest:\")\n",
    "for i in range(len(integers)-10, len(integers)):\n",
    "    print(integers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar7863273401093534368/] [] /tmp/streamjob4466524454374909242.jar tmpDir=null\n",
      "16/01/26 14:45:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:45:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:45:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 14:45:35 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/26 14:45:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453815382601_0012\n",
      "16/01/26 14:45:35 INFO impl.YarnClientImpl: Submitted application application_1453815382601_0012\n",
      "16/01/26 14:45:35 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0012/\n",
      "16/01/26 14:45:35 INFO mapreduce.Job: Running job: job_1453815382601_0012\n",
      "16/01/26 14:45:40 INFO mapreduce.Job: Job job_1453815382601_0012 running in uber mode : false\n",
      "16/01/26 14:45:40 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 14:45:47 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 14:45:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 14:45:52 INFO mapreduce.Job: Job job_1453815382601_0012 completed successfully\n",
      "16/01/26 14:45:52 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=77750\n",
      "\t\tFILE: Number of bytes written=508647\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=70942\n",
      "\t\tHDFS: Number of bytes written=125\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9723\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3181\n",
      "\t\tTotal time spent by all map tasks (ms)=9723\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3181\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9723\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3181\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9956352\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3257344\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=57744\n",
      "\t\tMap output materialized bytes=77756\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4299\n",
      "\t\tReduce shuffle bytes=77756\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=25\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=192\n",
      "\t\tCPU time spent (ms)=2990\n",
      "\t\tPhysical memory (bytes) snapshot=743747584\n",
      "\t\tVirtual memory (bytes) snapshot=2513383424\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=70736\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=125\n",
      "16/01/26 14:45:52 INFO streaming.StreamJob: Output directory: /user/ubuntu/sort/output\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=-k1nr \\\n",
    "-mapper /home/ubuntu/repos/261/HW2/mapper.py \\\n",
    "-reducer /home/ubuntu/repos/261/HW2/reducer.py \\\n",
    "-input /user/ubuntu/sort/integers.txt \\\n",
    "-output /user/ubuntu/sort/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-01-26 14:45 /user/ubuntu/sort/output/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu supergroup        125 2016-01-26 14:45 /user/ubuntu/sort/output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -ls /user/ubuntu/sort/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten biggest:\t\r\n",
      "4998\t\r\n",
      "4998\t\r\n",
      "4998\t\r\n",
      "4997\t\r\n",
      "4996\t\r\n",
      "4996\t\r\n",
      "4996\t\r\n",
      "4995\t\r\n",
      "4995\t\r\n",
      "4995\t\r\n",
      "\t\r\n",
      "\t\r\n",
      "\t\r\n",
      "Ten smallest:\t\r\n",
      "4\t\r\n",
      "4\t\r\n",
      "4\t\r\n",
      "3\t\r\n",
      "3\t\r\n",
      "2\t\r\n",
      "2\t\r\n",
      "1\t\r\n",
      "1\t\r\n",
      "0\t\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/sort/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.2\n",
    "\n",
    "Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re\n",
    "# input comes from STDIN (standard input)\n",
    "findword = sys.argv[1]\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    for word in [x.lower() for x in WORD_RE.findall(line)]:\n",
    "        if word == findword:\n",
    "            print(word+\", 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "currentWord= None\n",
    "currentCount = 0\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split(\",\")\n",
    "    if currentWord is None:\n",
    "        currentWord = word.lower()\n",
    "    if word.lower() == currentWord:\n",
    "        currentCount += int(count.rstrip())\n",
    "    else:\n",
    "        print(currentWord + \" \" + str(currentCount))\n",
    "        currentWord = word\n",
    "        currentCount = 0\n",
    "print(currentWord + \" \" + str(currentCount))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar5352705640018163427/] [] /tmp/streamjob269065301147711920.jar tmpDir=null\n",
      "16/01/26 14:46:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:46:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:46:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 14:46:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/26 14:46:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453815382601_0013\n",
      "16/01/26 14:46:39 INFO impl.YarnClientImpl: Submitted application application_1453815382601_0013\n",
      "16/01/26 14:46:39 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0013/\n",
      "16/01/26 14:46:39 INFO mapreduce.Job: Running job: job_1453815382601_0013\n",
      "16/01/26 14:46:44 INFO mapreduce.Job: Job job_1453815382601_0013 running in uber mode : false\n",
      "16/01/26 14:46:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 14:46:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 14:46:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 14:46:56 INFO mapreduce.Job: Job job_1453815382601_0013 completed successfully\n",
      "16/01/26 14:46:56 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=176\n",
      "\t\tFILE: Number of bytes written=352464\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216885\n",
      "\t\tHDFS: Number of bytes written=15\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9592\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2889\n",
      "\t\tTotal time spent by all map tasks (ms)=9592\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2889\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9592\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2889\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9822208\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2958336\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=150\n",
      "\t\tMap output materialized bytes=182\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=182\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=129\n",
      "\t\tCPU time spent (ms)=1650\n",
      "\t\tPhysical memory (bytes) snapshot=722419712\n",
      "\t\tVirtual memory (bytes) snapshot=2495299584\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=15\n",
      "16/01/26 14:46:56 INFO streaming.StreamJob: Output directory: /user/ubuntu/wordcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper \"/home/ubuntu/repos/261/HW2/mapper.py assistance\" \\\n",
    "-reducer /home/ubuntu/repos/261/HW2/reducer.py \\\n",
    "-input /user/ubuntu/enronemail_1h.txt \\\n",
    "-output /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-01-26 14:46 /user/ubuntu/wordcountOutput/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu supergroup         15 2016-01-26 14:46 /user/ubuntu/wordcountOutput/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -ls /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance 10\t\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.2.1\n",
    "\n",
    "Finding the top ten most common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys, re\n",
    "# input comes from STDIN (standard input)\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    for word in [x.lower() for x in WORD_RE.findall(line)]:\n",
    "        print(word+\", 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys, operator\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "counts = {}\n",
    "\n",
    "currentWord= None\n",
    "currentCount = 0\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split(\",\")\n",
    "    if currentWord is None:\n",
    "        currentWord = word.lower()\n",
    "    if word.lower() == currentWord:\n",
    "        currentCount += int(count.rstrip())\n",
    "    else:\n",
    "        counts[currentWord] = currentCount\n",
    "        currentWord = word\n",
    "        currentCount = int(count.rstrip())\n",
    "counts[currentWord] = currentCount\n",
    "\n",
    "\n",
    "print(\"Top ten most common tokens:\")\n",
    "for key, value in sorted(counts.items(), key=operator.itemgetter(1), reverse=True)[:10]:\n",
    "    print(key + \" \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.txt\n",
    "hat, 10\n",
    "hat, 5\n",
    "cat, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten most common tokens:\r\n",
      "hat 15\r\n",
      "cat 6\r\n"
     ]
    }
   ],
   "source": [
    "!cat test.txt | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 14:47:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/ubuntu/wordcountOutput/_SUCCESS\r\n",
      "16/01/26 14:47:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/ubuntu/wordcountOutput/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -rm /user/ubuntu/wordcountOutput/*\n",
    "!hadoop/bin/hdfs dfs -rmdir /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar6974803496656650750/] [] /tmp/streamjob9106210242066941131.jar tmpDir=null\n",
      "16/01/26 14:47:48 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:47:48 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:47:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 14:47:48 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/26 14:47:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453815382601_0015\n",
      "16/01/26 14:47:48 INFO impl.YarnClientImpl: Submitted application application_1453815382601_0015\n",
      "16/01/26 14:47:48 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0015/\n",
      "16/01/26 14:47:48 INFO mapreduce.Job: Running job: job_1453815382601_0015\n",
      "16/01/26 14:47:54 INFO mapreduce.Job: Job job_1453815382601_0015 running in uber mode : false\n",
      "16/01/26 14:47:54 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 14:48:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 14:48:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 14:48:06 INFO mapreduce.Job: Job job_1453815382601_0015 completed successfully\n",
      "16/01/26 14:48:06 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=389923\n",
      "\t\tFILE: Number of bytes written=1131928\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216885\n",
      "\t\tHDFS: Number of bytes written=116\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9513\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3111\n",
      "\t\tTotal time spent by all map tasks (ms)=9513\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3111\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9513\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3111\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9741312\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3185664\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=33511\n",
      "\t\tMap output bytes=322895\n",
      "\t\tMap output materialized bytes=389929\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5511\n",
      "\t\tReduce shuffle bytes=389929\n",
      "\t\tReduce input records=33511\n",
      "\t\tReduce output records=11\n",
      "\t\tSpilled Records=67022\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=112\n",
      "\t\tCPU time spent (ms)=3280\n",
      "\t\tPhysical memory (bytes) snapshot=710307840\n",
      "\t\tVirtual memory (bytes) snapshot=2490212352\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=116\n",
      "16/01/26 14:48:06 INFO streaming.StreamJob: Output directory: /user/ubuntu/wordcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/ubuntu/repos/261/HW2/mapper.py \\\n",
    "-reducer /home/ubuntu/repos/261/HW2/reducer.py \\\n",
    "-input /user/ubuntu/enronemail_1h.txt \\\n",
    "-output /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-01-26 14:48 /user/ubuntu/wordcountOutput/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu supergroup        116 2016-01-26 14:48 /user/ubuntu/wordcountOutput/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -ls /user/ubuntu/wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten most common tokens:\t\r\n",
      "the 1247\t\r\n",
      "to 963\t\r\n",
      "and 668\t\r\n",
      "of 566\t\r\n",
      "a 542\t\r\n",
      "you 432\t\r\n",
      "in 417\t\r\n",
      "your 394\t\r\n",
      "ect 382\t\r\n",
      "for 373\t\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.3\n",
    "Multinomial Naive Bayes without smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys, re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    if len(components) < 4:\n",
    "        continue\n",
    "    ID = components[0]\n",
    "    flag = components[1]\n",
    "    text = components[2] + \" \" + components[3]\n",
    "    word_counts = {}\n",
    "    #do a little in-mapper combining\n",
    "    for word in WORD_RE.findall(text):\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "    for word, count in word_counts.items():\n",
    "        print(ID + \"\\t\" + str(flag) + \"\\t\" + word + \"\\t\" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys, re\n",
    "from math import log\n",
    "\n",
    "#for calculating priors\n",
    "emails = set()\n",
    "spams = set()\n",
    "#for calculating cond probs\n",
    "words_in_corpus = set()\n",
    "spam_counts = {}\n",
    "ham_counts = {}\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    splits = re.split('\\t', line)\n",
    "    if len(splits) < 4:\n",
    "        continue\n",
    "    emailID = splits[0]\n",
    "    label = splits[1]\n",
    "    word = splits[2]\n",
    "    count = int(splits[3].rstrip())\n",
    "\n",
    "    \n",
    "    emails.add(emailID)\n",
    "    words_in_corpus.add(word)\n",
    "    \n",
    "    #if the email is spam, add it to the list of spams\n",
    "    #and add all the words in it to the spam counts\n",
    "    if (label == '1'):\n",
    "        spams.add(emailID)\n",
    "        if (word not in spam_counts):\n",
    "            spam_counts[word] = count\n",
    "        else:\n",
    "            spam_counts[word] += count\n",
    "    #if it's not spam, add the words to the ham counts\n",
    "    else:\n",
    "        if (word not in ham_counts):\n",
    "            ham_counts[word] = count\n",
    "        else:\n",
    "            ham_counts[word] += count\n",
    "\n",
    "            \n",
    "prior_spam = 1.*len(spams)/len(emails)\n",
    "prior_ham = 1.*(len(emails) - len(spams))/len(emails)\n",
    "condprob_ham = {}\n",
    "condprob_spam = {}\n",
    "\n",
    "#with no smoothing, words that do not occur in class c do not contribute to the denominator \n",
    "#of the conditional probability.  Words that do not occur in class c have a probability of 0\n",
    "#which will have to be accounted for in the classification stage.\n",
    "for word in words_in_corpus:\n",
    "    if word in spam_counts:\n",
    "        condprob_spam[word] = 1.*(spam_counts[word])/sum(spam_counts.values())\n",
    "    else:\n",
    "        condprob_spam[word] = 0\n",
    "    if word in ham_counts:\n",
    "        condprob_ham[word] = 1.*(ham_counts[word])/(sum(ham_counts.values()))\n",
    "        \n",
    "    else:\n",
    "        condprob_ham[word] = 0\n",
    "        \n",
    "print(prior_spam)\n",
    "print(prior_ham)\n",
    "print(str(len(condprob_spam)))\n",
    "for word, condprob in condprob_spam.items():\n",
    "    print(word + \":\" + str(condprob))\n",
    "print(str(len(condprob_ham)))\n",
    "for word, condprob in condprob_ham.items():\n",
    "    print(word + \":\" + str(condprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar756474697208507481/] [] /tmp/streamjob2754028478183952815.jar tmpDir=null\n",
      "16/01/26 14:48:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:48:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:48:46 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 14:48:46 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/26 14:48:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453815382601_0016\n",
      "16/01/26 14:48:46 INFO impl.YarnClientImpl: Submitted application application_1453815382601_0016\n",
      "16/01/26 14:48:46 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0016/\n",
      "16/01/26 14:48:46 INFO mapreduce.Job: Running job: job_1453815382601_0016\n",
      "16/01/26 14:48:51 INFO mapreduce.Job: Job job_1453815382601_0016 running in uber mode : false\n",
      "16/01/26 14:48:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 14:48:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 14:49:04 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 14:49:04 INFO mapreduce.Job: Job job_1453815382601_0016 completed successfully\n",
      "16/01/26 14:49:04 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=538693\n",
      "\t\tFILE: Number of bytes written=1429447\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216885\n",
      "\t\tHDFS: Number of bytes written=218351\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9123\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3664\n",
      "\t\tTotal time spent by all map tasks (ms)=9123\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3664\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9123\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3664\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9341952\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3751936\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=15112\n",
      "\t\tMap output bytes=508463\n",
      "\t\tMap output materialized bytes=538699\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=98\n",
      "\t\tReduce shuffle bytes=538699\n",
      "\t\tReduce input records=15112\n",
      "\t\tReduce output records=10870\n",
      "\t\tSpilled Records=30224\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=99\n",
      "\t\tCPU time spent (ms)=3070\n",
      "\t\tPhysical memory (bytes) snapshot=722350080\n",
      "\t\tVirtual memory (bytes) snapshot=2499534848\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=218351\n",
      "16/01/26 14:49:04 INFO streaming.StreamJob: Output directory: /user/ubuntu/NBOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/ubuntu/repos/261/HW2/mapper.py \\\n",
    "-reducer /home/ubuntu/repos/261/HW2/reducer.py \\\n",
    "-input /user/ubuntu/enronemail_1h.txt \\\n",
    "-output /user/ubuntu/NBOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu supergroup          0 2016-01-26 13:47 /user/ubuntu/NBOutput/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu supergroup     218351 2016-01-26 13:47 /user/ubuntu/NBOutput/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -ls /user/ubuntu/NBOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.438775510204\t\r\n",
      "0.561224489796\t\r\n",
      "5433\t\r\n",
      "stock:0.000109781534746\t\r\n",
      "limited:0.000494016906356\t\r\n",
      "entergyr:0\t\r\n",
      "paul:0.000109781534746\t\r\n",
      "believed:0\t\r\n",
      "child:0.000109781534746\t\r\n",
      "dynamic:0\t\r\n",
      "knelt:5.48907673729e-05\t\r\n",
      "yellow:5.48907673729e-05\t\r\n",
      "four:0.000274453836865\t\r\n",
      "protest:0\t\r\n",
      "sleep:0.000109781534746\t\r\n",
      "invovled:0\t\r\n",
      "'mperkins:0\t\r\n",
      "railing:5.48907673729e-05\t\r\n",
      "appetite:0.000109781534746\t\r\n",
      "hate:0.000219563069492\t\r\n",
      "forget:5.48907673729e-05\t\r\n",
      "whose:5.48907673729e-05\t\r\n",
      "fronts:0\t\r\n",
      "chinese:5.48907673729e-05\t\r\n",
      "granting:5.48907673729e-05\t\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/NBOutput/part-00000 | head -25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 14:49:26 WARN hdfs.DFSClient: DFSInputStream has been closed already\r\n"
     ]
    }
   ],
   "source": [
    "!rm probs.txt\n",
    "!hadoop/bin/hdfs dfs -get /user/ubuntu/NBOutput/part-00000 ./probs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys, re\n",
    "from math import log\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "probs = open(sys.argv[1], 'r')\n",
    "#probs = open(\"probs.txt\", 'r')\n",
    "prior_spam = float(probs.readline().rstrip())\n",
    "prior_ham = float(probs.readline().rstrip())\n",
    "\n",
    "condprobs_spam = {}\n",
    "condprobs_ham = {}\n",
    "\n",
    "ham_ignores = 0\n",
    "spam_ignores = 0\n",
    "\n",
    "num_condprobs_spam = int(probs.readline().rstrip())\n",
    "for i in range(num_condprobs_spam):\n",
    "    splits = re.split(':', probs.readline().rstrip())\n",
    "    if len(splits) != 2:\n",
    "        continue\n",
    "    condprobs_spam[splits[0]] = float(splits[1])\n",
    "    \n",
    "num_condprobs_ham = int(probs.readline().rstrip())\n",
    "for i in range(num_condprobs_ham):\n",
    "    splits = re.split(':', probs.readline().rstrip())\n",
    "    if len(splits) != 2:\n",
    "        continue\n",
    "    condprobs_ham[splits[0]] = float(splits[1])\n",
    "    \n",
    "    \n",
    "email_count = 0\n",
    "correct_guesses = 0\n",
    "for line in sys.stdin:\n",
    "    email_count += 1\n",
    "    splits = re.split(\"\\t\", line)\n",
    "    if len(splits) != 4:\n",
    "        continue\n",
    "    emailID = splits[0]\n",
    "    label = splits[1]\n",
    "    text = splits[2] + \" \" + splits[3]\n",
    "    local_prob_spam = log(prior_spam)\n",
    "    local_prob_ham = log(prior_ham)\n",
    "    classification = 0\n",
    "    printed = False\n",
    "    for word in WORD_RE.findall(text):\n",
    "        if condprobs_ham[word] > 0:\n",
    "            local_prob_ham += log(condprobs_ham[word])\n",
    "        else:\n",
    "            print(emailID + \"\\t\" + str(label) + \"\\t\" + str(1))\n",
    "            if label == '1':\n",
    "                correct_guesses += 1\n",
    "            ham_ignores += 1\n",
    "            printed = True\n",
    "            break\n",
    "        if condprobs_spam[word] > 0:\n",
    "            local_prob_spam += log(condprobs_spam[word])\n",
    "        else:\n",
    "            print(emailID + \"\\t\" + str(label) + \"\\t\" + str(0))\n",
    "            if label == '0':\n",
    "                correct_guesses += 1\n",
    "            spam_ignores += 1\n",
    "            printed = True\n",
    "            break\n",
    "    if printed:\n",
    "        continue\n",
    "    if local_prob_spam < local_prob_ham:\n",
    "        classification = 1\n",
    "    print(emailID + \"\\t\" + str(label) + \"\\t\" + str(classification))\n",
    "print(\"\")   \n",
    "print(\"Accuracy: \" + str(1.*correct_guesses/email_count))\n",
    "print(\"Ham zeros: \" + str(ham_ignores))\n",
    "print(\"Spam zeros: \" + str(spam_ignores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 13:49:04 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/ubuntu/repos/261/HW2/probs.txt, /tmp/hadoop-unjar5033776292472386847/] [] /tmp/streamjob124938863642221283.jar tmpDir=null\n",
      "16/01/26 13:49:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 13:49:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 13:49:05 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 13:49:05 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/26 13:49:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453815382601_0006\n",
      "16/01/26 13:49:06 INFO impl.YarnClientImpl: Submitted application application_1453815382601_0006\n",
      "16/01/26 13:49:06 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0006/\n",
      "16/01/26 13:49:06 INFO mapreduce.Job: Running job: job_1453815382601_0006\n",
      "16/01/26 13:49:11 INFO mapreduce.Job: Job job_1453815382601_0006 running in uber mode : false\n",
      "16/01/26 13:49:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 13:49:17 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 13:49:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 13:49:23 INFO mapreduce.Job: Job job_1453815382601_0006 completed successfully\n",
      "16/01/26 13:49:23 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2941\n",
      "\t\tFILE: Number of bytes written=359326\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216885\n",
      "\t\tHDFS: Number of bytes written=2723\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8570\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2735\n",
      "\t\tTotal time spent by all map tasks (ms)=8570\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2735\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8570\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2735\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8775680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2800640\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=106\n",
      "\t\tMap output bytes=2723\n",
      "\t\tMap output materialized bytes=2947\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=105\n",
      "\t\tReduce shuffle bytes=2947\n",
      "\t\tReduce input records=106\n",
      "\t\tReduce output records=106\n",
      "\t\tSpilled Records=212\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=106\n",
      "\t\tCPU time spent (ms)=1530\n",
      "\t\tPhysical memory (bytes) snapshot=725942272\n",
      "\t\tVirtual memory (bytes) snapshot=2493255680\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2723\n",
      "16/01/26 13:49:23 INFO streaming.StreamJob: Output directory: /user/ubuntu/NBOutputStage2\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper \"/home/ubuntu/repos/261/HW2/mapper.py /home/ubuntu/repos/261/HW2/probs.txt\" \\\n",
    "-file /home/ubuntu/repos/261/HW2/probs.txt \\\n",
    "-input /user/ubuntu/enronemail_1h.txt \\\n",
    "-output /user/ubuntu/NBOutputStage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\r\n",
      "\t\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accuracy: 0.950819672131\t\r\n",
      "Accuracy: 1.0\t\r\n",
      "Ham zeros: 19\t\r\n",
      "Ham zeros: 24\t\r\n",
      "Spam zeros: 21\t\r\n",
      "Spam zeros: 34\t\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/NBOutputStage2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.4\n",
    "\n",
    "Multinomial Naive Bayes with Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys, re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    if len(components) < 4:\n",
    "        continue\n",
    "    ID = components[0]\n",
    "    flag = components[1]\n",
    "    text = components[2] + \" \" + components[3]\n",
    "    word_counts = {}\n",
    "    #do a little in-mapper combining\n",
    "    for word in WORD_RE.findall(text):\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "    for word, count in word_counts.items():\n",
    "        print(ID + \"\\t\" + str(flag) + \"\\t\" + word + \"\\t\" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys, re\n",
    "from math import log\n",
    "\n",
    "#for calculating priors\n",
    "emails = set()\n",
    "spams = set()\n",
    "#for calculating cond probs\n",
    "words_in_corpus = set()\n",
    "spam_counts = {}\n",
    "ham_counts = {}\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    splits = re.split('\\t', line)\n",
    "    if len(splits) < 4:\n",
    "        continue\n",
    "    emailID = splits[0]\n",
    "    label = splits[1]\n",
    "    word = splits[2]\n",
    "    count = int(splits[3].rstrip())\n",
    "\n",
    "    \n",
    "    emails.add(emailID)\n",
    "    words_in_corpus.add(word)\n",
    "    \n",
    "    #if the email is spam, add it to the list of spams\n",
    "    #and add all the words in it to the spam counts\n",
    "    if (label == '1'):\n",
    "        spams.add(emailID)\n",
    "        if (word not in spam_counts):\n",
    "            spam_counts[word] = count\n",
    "        else:\n",
    "            spam_counts[word] += count\n",
    "    #if it's not spam, add the words to the ham counts\n",
    "    else:\n",
    "        if (word not in ham_counts):\n",
    "            ham_counts[word] = count\n",
    "        else:\n",
    "            ham_counts[word] += count\n",
    "\n",
    "            \n",
    "prior_spam = 1.*len(spams)/len(emails)\n",
    "prior_ham = 1.*(len(emails) - len(spams))/len(emails)\n",
    "condprob_ham = {}\n",
    "condprob_spam = {}\n",
    "\n",
    "#laplace smoothing\n",
    "for word in words_in_corpus:\n",
    "    if word in spam_counts:\n",
    "        condprob_spam[word] = 1.*(spam_counts[word] + 1)/(sum(spam_counts.values()) + len(words_in_corpus))\n",
    "    else:\n",
    "        condprob_spam[word] = 1.*(1)/(sum(spam_counts.values()) + len(words_in_corpus))\n",
    "    if word in ham_counts:\n",
    "        condprob_ham[word] = 1.*(ham_counts[word] + 1)/((sum(ham_counts.values())) + len(words_in_corpus))\n",
    "    else:\n",
    "        condprob_ham[word] = 1.*(1)/((sum(ham_counts.values())) + len(words_in_corpus))\n",
    "        \n",
    "print(prior_spam)\n",
    "print(prior_ham)\n",
    "print(str(len(condprob_spam)))\n",
    "for word, condprob in condprob_spam.items():\n",
    "    print(word + \":\" + str(condprob))\n",
    "print(str(len(condprob_ham)))\n",
    "for word, condprob in condprob_ham.items():\n",
    "    print(word + \":\" + str(condprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar1570661844122603360/] [] /tmp/streamjob9146369158596101409.jar tmpDir=null\n",
      "16/01/26 13:36:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 13:36:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 13:36:34 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 13:36:34 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/26 13:36:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453815382601_0001\n",
      "16/01/26 13:36:34 INFO impl.YarnClientImpl: Submitted application application_1453815382601_0001\n",
      "16/01/26 13:36:34 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0001/\n",
      "16/01/26 13:36:34 INFO mapreduce.Job: Running job: job_1453815382601_0001\n",
      "16/01/26 13:36:42 INFO mapreduce.Job: Job job_1453815382601_0001 running in uber mode : false\n",
      "16/01/26 13:36:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 13:36:49 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 13:36:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 13:36:55 INFO mapreduce.Job: Job job_1453815382601_0001 completed successfully\n",
      "16/01/26 13:36:55 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=538693\n",
      "\t\tFILE: Number of bytes written=1429468\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216885\n",
      "\t\tHDFS: Number of bytes written=287396\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9652\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3924\n",
      "\t\tTotal time spent by all map tasks (ms)=9652\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3924\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9652\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3924\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9883648\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4018176\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=15112\n",
      "\t\tMap output bytes=508463\n",
      "\t\tMap output materialized bytes=538699\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=98\n",
      "\t\tReduce shuffle bytes=538699\n",
      "\t\tReduce input records=15112\n",
      "\t\tReduce output records=10870\n",
      "\t\tSpilled Records=30224\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=148\n",
      "\t\tCPU time spent (ms)=2990\n",
      "\t\tPhysical memory (bytes) snapshot=718327808\n",
      "\t\tVirtual memory (bytes) snapshot=2498166784\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=287396\n",
      "16/01/26 13:36:55 INFO streaming.StreamJob: Output directory: /user/ubuntu/NBOutputLaplace\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/ubuntu/repos/261/HW2/mapper.py \\\n",
    "-reducer /home/ubuntu/repos/261/HW2/reducer.py \\\n",
    "-input /user/ubuntu/enronemail_1h.txt \\\n",
    "-output /user/ubuntu/NBOutputLaplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.438775510204\t\r\n",
      "0.561224489796\t\r\n",
      "5433\t\r\n",
      "stock:0.000126844530887\t\r\n",
      "limited:0.000422815102955\t\r\n",
      "entergyr:4.22815102955e-05\t\r\n",
      "paul:0.000126844530887\t\r\n",
      "believed:4.22815102955e-05\t\r\n",
      "child:0.000126844530887\t\r\n",
      "dynamic:4.22815102955e-05\t\r\n",
      "knelt:8.45630205911e-05\t\r\n",
      "yellow:8.45630205911e-05\t\r\n",
      "four:0.000253689061773\t\r\n",
      "protest:4.22815102955e-05\t\r\n",
      "sleep:0.000126844530887\t\r\n",
      "invovled:4.22815102955e-05\t\r\n",
      "'mperkins:4.22815102955e-05\t\r\n",
      "railing:8.45630205911e-05\t\r\n",
      "appetite:0.000126844530887\t\r\n",
      "hate:0.000211407551478\t\r\n",
      "forget:8.45630205911e-05\t\r\n",
      "whose:8.45630205911e-05\t\r\n",
      "fronts:4.22815102955e-05\t\r\n",
      "chinese:8.45630205911e-05\t\r\n",
      "granting:8.45630205911e-05\t\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/NBOutputLaplace/part-00000 | head -25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 13:39:21 WARN hdfs.DFSClient: DFSInputStream has been closed already\r\n"
     ]
    }
   ],
   "source": [
    "!rm probs.txt\n",
    "!hdfs dfs -get /user/ubuntu/NBOutputLaplace/part-00000 probs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys, re\n",
    "from math import log\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "probs = open(sys.argv[1], 'r')\n",
    "#probs = open(\"probs.txt\", 'r')\n",
    "prior_spam = float(probs.readline().rstrip())\n",
    "prior_ham = float(probs.readline().rstrip())\n",
    "\n",
    "condprobs_spam = {}\n",
    "condprobs_ham = {}\n",
    "\n",
    "ham_ignores = 0\n",
    "spam_ignores = 0\n",
    "\n",
    "num_condprobs_spam = int(probs.readline().rstrip())\n",
    "for i in range(num_condprobs_spam):\n",
    "    splits = re.split(':', probs.readline().rstrip())\n",
    "    if len(splits) != 2:\n",
    "        continue\n",
    "    condprobs_spam[splits[0]] = float(splits[1])\n",
    "    \n",
    "num_condprobs_ham = int(probs.readline().rstrip())\n",
    "for i in range(num_condprobs_ham):\n",
    "    splits = re.split(':', probs.readline().rstrip())\n",
    "    if len(splits) != 2:\n",
    "        continue\n",
    "    condprobs_ham[splits[0]] = float(splits[1])\n",
    "    \n",
    "    \n",
    "email_count = 0\n",
    "correct_guesses = 0\n",
    "for line in sys.stdin:\n",
    "    email_count += 1\n",
    "    splits = re.split(\"\\t\", line)\n",
    "    if len(splits) != 4:\n",
    "        continue\n",
    "    emailID = splits[0]\n",
    "    label = splits[1]\n",
    "    text = splits[2] + \" \" + splits[3]\n",
    "    local_prob_spam = log(prior_spam)\n",
    "    local_prob_ham = log(prior_ham)\n",
    "    classification = 0\n",
    "    printed = False\n",
    "    for word in WORD_RE.findall(text):\n",
    "        if condprobs_ham[word] > 0:\n",
    "            local_prob_ham += log(condprobs_ham[word])\n",
    "        else:\n",
    "            print(emailID + \"\\t\" + str(label) + \"\\t\" + str(1))\n",
    "            if label == '1':\n",
    "                correct_guesses += 1\n",
    "            ham_ignores += 1\n",
    "            printed = True\n",
    "            break\n",
    "        if condprobs_spam[word] > 0:\n",
    "            local_prob_spam += log(condprobs_spam[word])\n",
    "        else:\n",
    "            print(emailID + \"\\t\" + str(label) + \"\\t\" + str(0))\n",
    "            if label == '0':\n",
    "                correct_guesses += 1\n",
    "            spam_ignores += 1\n",
    "            printed = True\n",
    "            break\n",
    "    if printed:\n",
    "        continue\n",
    "    if local_prob_spam > local_prob_ham:\n",
    "        classification = 1\n",
    "    if str(classification) == label:\n",
    "        correct_guesses += 1\n",
    "    print(emailID + \"\\t\" + str(label) + \"\\t\" + str(classification))\n",
    "print(\"\")   \n",
    "print(\"Accuracy: \" + str(1.*correct_guesses/email_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 13:45:17 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/ubuntu/repos/261/HW2/probs.txt, /tmp/hadoop-unjar3497387638049298110/] [] /tmp/streamjob6819139917180522715.jar tmpDir=null\n",
      "16/01/26 13:45:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 13:45:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 13:45:18 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 13:45:19 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/26 13:45:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453815382601_0004\n",
      "16/01/26 13:45:19 INFO impl.YarnClientImpl: Submitted application application_1453815382601_0004\n",
      "16/01/26 13:45:19 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0004/\n",
      "16/01/26 13:45:19 INFO mapreduce.Job: Running job: job_1453815382601_0004\n",
      "16/01/26 13:45:25 INFO mapreduce.Job: Job job_1453815382601_0004 running in uber mode : false\n",
      "16/01/26 13:45:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 13:45:31 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 13:45:36 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 13:45:37 INFO mapreduce.Job: Job job_1453815382601_0004 completed successfully\n",
      "16/01/26 13:45:37 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2871\n",
      "\t\tFILE: Number of bytes written=359210\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216885\n",
      "\t\tHDFS: Number of bytes written=2661\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8858\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2839\n",
      "\t\tTotal time spent by all map tasks (ms)=8858\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2839\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8858\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2839\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9070592\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2907136\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=102\n",
      "\t\tMap output bytes=2661\n",
      "\t\tMap output materialized bytes=2877\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=101\n",
      "\t\tReduce shuffle bytes=2877\n",
      "\t\tReduce input records=102\n",
      "\t\tReduce output records=102\n",
      "\t\tSpilled Records=204\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=104\n",
      "\t\tCPU time spent (ms)=1760\n",
      "\t\tPhysical memory (bytes) snapshot=733945856\n",
      "\t\tVirtual memory (bytes) snapshot=2513215488\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2661\n",
      "16/01/26 13:45:37 INFO streaming.StreamJob: Output directory: /user/ubuntu/NBOutputLaplaceStage2\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper \"/home/ubuntu/repos/261/HW2/mapper.py /home/ubuntu/repos/261/HW2/probs.txt\" \\\n",
    "-file /home/ubuntu/repos/261/HW2/probs.txt \\\n",
    "-input /user/ubuntu/enronemail_1h.txt \\\n",
    "-output /user/ubuntu/NBOutputLaplaceStage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\r\n",
      "\t\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accuracy: 0.950819672131\t\r\n",
      "Accuracy: 1.0\t\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/NBOutputLaplaceStage2/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 13:45:11 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/ubuntu/NBOutputLaplaceStage2/_SUCCESS\r\n",
      "16/01/26 13:45:11 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/ubuntu/NBOutputLaplaceStage2/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -rm /user/ubuntu/NBOutputLaplaceStage2/*\n",
    "!hadoop/bin/hdfs dfs -rmdir /user/ubuntu/NBOutputLaplaceStage2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy rate is about the same.  This is likely because testing and training on the same data allows us to recognize that zeroing out class probability based on unobserved words is its own kind of smoothing; rather than assigning a small prior for each word, not observing a word is effectively a perfectly accurate prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.5\n",
    "\n",
    "Ignore words with a frequency of less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys, re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    if len(components) < 4:\n",
    "        continue\n",
    "    ID = components[0]\n",
    "    flag = components[1]\n",
    "    text = components[2] + \" \" + components[3]\n",
    "    word_counts = {}\n",
    "    #do a little in-mapper combining\n",
    "    for word in WORD_RE.findall(text):\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "    for word, count in word_counts.items():\n",
    "        print(ID + \"\\t\" + str(flag) + \"\\t\" + word + \"\\t\" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys, re\n",
    "from math import log\n",
    "\n",
    "#for calculating priors\n",
    "emails = set()\n",
    "spams = set()\n",
    "#for calculating cond probs\n",
    "words_in_corpus = set()\n",
    "spam_counts = {}\n",
    "ham_counts = {}\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    splits = re.split('\\t', line)\n",
    "    if len(splits) < 4:\n",
    "        continue\n",
    "    emailID = splits[0]\n",
    "    label = splits[1]\n",
    "    word = splits[2]\n",
    "    count = int(splits[3].rstrip())\n",
    "\n",
    "    \n",
    "    emails.add(emailID)\n",
    "    words_in_corpus.add(word)\n",
    "    \n",
    "    #if the email is spam, add it to the list of spams\n",
    "    #and add all the words in it to the spam counts\n",
    "    if (label == '1'):\n",
    "        spams.add(emailID)\n",
    "        if (word not in spam_counts):\n",
    "            spam_counts[word] = count\n",
    "        else:\n",
    "            spam_counts[word] += count\n",
    "    #if it's not spam, add the words to the ham counts\n",
    "    else:\n",
    "        if (word not in ham_counts):\n",
    "            ham_counts[word] = count\n",
    "        else:\n",
    "            ham_counts[word] += count\n",
    "\n",
    "            \n",
    "prior_spam = 1.*len(spams)/len(emails)\n",
    "prior_ham = 1.*(len(emails) - len(spams))/len(emails)\n",
    "condprob_ham = {}\n",
    "condprob_spam = {}\n",
    "\n",
    "for word in words_in_corpus:\n",
    "    #ignore a word with fewer than 3 occurrences\n",
    "    tokens_in_spam = 0\n",
    "    tokens_in_ham = 0\n",
    "    if word in spam_counts:\n",
    "        tokens_in_spam += spam_counts[word]\n",
    "    if word in ham_counts:\n",
    "        tokens_in_ham += ham_counts[word]\n",
    "    if tokens_in_spam + tokens_in_ham < 3:\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    if word in spam_counts:\n",
    "        condprob_spam[word] = 1.*(spam_counts[word] + 1)/(sum(spam_counts.values()) + len(words_in_corpus))\n",
    "    else:\n",
    "        condprob_spam[word] = 1.*(1)/(sum(spam_counts.values()) + len(words_in_corpus))\n",
    "    if word in ham_counts:\n",
    "        condprob_ham[word] = 1.*(ham_counts[word] + 1)/((sum(ham_counts.values())) + len(words_in_corpus))\n",
    "    else:\n",
    "        condprob_ham[word] = 1.*(1)/((sum(ham_counts.values())) + len(words_in_corpus))\n",
    "        \n",
    "print(prior_spam)\n",
    "print(prior_ham)\n",
    "print(str(len(condprob_spam)))\n",
    "for word, condprob in condprob_spam.items():\n",
    "    print(word + \":\" + str(condprob))\n",
    "print(str(len(condprob_ham)))\n",
    "for word, condprob in condprob_ham.items():\n",
    "    print(word + \":\" + str(condprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar3016643321423112512/] [] /tmp/streamjob6770305482498697858.jar tmpDir=null\n",
      "16/01/26 14:11:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:11:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:11:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 14:11:44 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/26 14:11:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453815382601_0008\n",
      "16/01/26 14:11:45 INFO impl.YarnClientImpl: Submitted application application_1453815382601_0008\n",
      "16/01/26 14:11:45 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0008/\n",
      "16/01/26 14:11:45 INFO mapreduce.Job: Running job: job_1453815382601_0008\n",
      "16/01/26 14:11:51 INFO mapreduce.Job: Job job_1453815382601_0008 running in uber mode : false\n",
      "16/01/26 14:11:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 14:11:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 14:12:03 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 14:12:03 INFO mapreduce.Job: Job job_1453815382601_0008 completed successfully\n",
      "16/01/26 14:12:03 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=538693\n",
      "\t\tFILE: Number of bytes written=1429462\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216885\n",
      "\t\tHDFS: Number of bytes written=94325\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8514\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3426\n",
      "\t\tTotal time spent by all map tasks (ms)=8514\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3426\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8514\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3426\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8718336\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3508224\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=15112\n",
      "\t\tMap output bytes=508463\n",
      "\t\tMap output materialized bytes=538699\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=98\n",
      "\t\tReduce shuffle bytes=538699\n",
      "\t\tReduce input records=15112\n",
      "\t\tReduce output records=3660\n",
      "\t\tSpilled Records=30224\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=113\n",
      "\t\tCPU time spent (ms)=2940\n",
      "\t\tPhysical memory (bytes) snapshot=715431936\n",
      "\t\tVirtual memory (bytes) snapshot=2491633664\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=94325\n",
      "16/01/26 14:12:03 INFO streaming.StreamJob: Output directory: /user/ubuntu/NBOutputDrop3\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/ubuntu/repos/261/HW2/mapper.py \\\n",
    "-reducer /home/ubuntu/repos/261/HW2/reducer.py \\\n",
    "-input /user/ubuntu/enronemail_1h.txt \\\n",
    "-output /user/ubuntu/NBOutputDrop3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.438775510204\t\r\n",
      "0.561224489796\t\r\n",
      "1828\t\r\n",
      "limited:0.000422815102955\t\r\n",
      "four:0.000253689061773\t\r\n",
      "hate:0.000211407551478\t\r\n",
      "looking:0.000211407551478\t\r\n",
      "bailout:4.22815102955e-05\t\r\n",
      "whatsoever:0.000169126041182\t\r\n",
      "under:0.000422815102955\t\r\n",
      "merchant:0.000169126041182\t\r\n",
      "collaborate:4.22815102955e-05\t\r\n",
      "risk:0.000169126041182\t\r\n",
      "regional:4.22815102955e-05\t\r\n",
      "every:0.000634222654433\t\r\n",
      "we'll:0.000507378123547\t\r\n",
      "withers:4.22815102955e-05\t\r\n",
      "companies:0.000169126041182\t\r\n",
      "wednesday:4.22815102955e-05\t\r\n",
      "enhance:4.22815102955e-05\t\r\n",
      "force:0.000211407551478\t\r\n",
      "direct:8.45630205911e-05\t\r\n",
      "tires:0.000169126041182\t\r\n",
      "dish:0.000169126041182\t\r\n",
      "likely:4.22815102955e-05\t\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/NBOutputDrop3/part-00000 | head -25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys, re\n",
    "from math import log\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "probs = open(sys.argv[1], 'r')\n",
    "prior_spam = float(probs.readline().rstrip())\n",
    "prior_ham = float(probs.readline().rstrip())\n",
    "condprobs_spam = {}\n",
    "condprobs_ham = {}\n",
    "\n",
    "ham_ignores = 0\n",
    "spam_ignores = 0\n",
    "\n",
    "num_condprobs_spam = int(probs.readline().rstrip())\n",
    "for i in range(num_condprobs_spam):\n",
    "    splits = re.split(':', probs.readline().rstrip())\n",
    "    if len(splits) != 2:\n",
    "        continue\n",
    "    condprobs_spam[splits[0]] = float(splits[1])\n",
    "    \n",
    "num_condprobs_ham = int(probs.readline().rstrip())\n",
    "for i in range(num_condprobs_ham):\n",
    "    splits = re.split(':', probs.readline().rstrip())\n",
    "    if len(splits) != 2:\n",
    "        continue\n",
    "    condprobs_ham[splits[0]] = float(splits[1])\n",
    "    \n",
    "    \n",
    "email_count = 0\n",
    "correct_guesses = 0\n",
    "for line in sys.stdin:\n",
    "    email_count += 1\n",
    "    splits = re.split(\"\\t\", line)\n",
    "    if len(splits) != 4:\n",
    "        continue\n",
    "    emailID = splits[0]\n",
    "    label = splits[1]\n",
    "    text = splits[2] + \" \" + splits[3]\n",
    "    local_prob_spam = log(prior_spam)\n",
    "    local_prob_ham = log(prior_ham)\n",
    "    classification = 0\n",
    "    printed = False\n",
    "    for word in WORD_RE.findall(text):\n",
    "        \n",
    "        if word in condprobs_ham and condprobs_ham[word] > 0:\n",
    "            local_prob_ham += log(condprobs_ham[word])\n",
    "        else:\n",
    "            print(emailID + \"\\t\" + str(label) + \"\\t\" + str(1))\n",
    "            if label == '1':\n",
    "                correct_guesses += 1\n",
    "            ham_ignores += 1\n",
    "            printed = True\n",
    "            break\n",
    "        if word in condprobs_spam and condprobs_spam[word] > 0:\n",
    "            local_prob_spam += log(condprobs_spam[word])\n",
    "        else:\n",
    "            print(emailID + \"\\t\" + str(label) + \"\\t\" + str(0))\n",
    "            if label == '0':\n",
    "                correct_guesses += 1\n",
    "            spam_ignores += 1\n",
    "            printed = True\n",
    "            break\n",
    "    if printed:\n",
    "        continue\n",
    "    if local_prob_spam > local_prob_ham:\n",
    "        classification = 1\n",
    "    if str(classification) == label:\n",
    "        correct_guesses += 1\n",
    "    print(emailID + \"\\t\" + str(label) + \"\\t\" + str(classification))\n",
    "print(\"\")   \n",
    "print(\"Accuracy: \" + str(1.*correct_guesses/email_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 14:13:17 WARN hdfs.DFSClient: DFSInputStream has been closed already\r\n"
     ]
    }
   ],
   "source": [
    "!rm probs.txt\n",
    "!hdfs dfs -get /user/ubuntu/NBOutputDrop3/part-00000 probs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 14:25:11 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/home/ubuntu/repos/261/HW2/probs.txt, /tmp/hadoop-unjar5699698884738858490/] [] /tmp/streamjob8709999309365559295.jar tmpDir=null\n",
      "16/01/26 14:25:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:25:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 14:25:12 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 14:25:12 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/26 14:25:12 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453815382601_0011\n",
      "16/01/26 14:25:13 INFO impl.YarnClientImpl: Submitted application application_1453815382601_0011\n",
      "16/01/26 14:25:13 INFO mapreduce.Job: The url to track the job: http://ip-172-31-54-132:8088/proxy/application_1453815382601_0011/\n",
      "16/01/26 14:25:13 INFO mapreduce.Job: Running job: job_1453815382601_0011\n",
      "16/01/26 14:25:18 INFO mapreduce.Job: Job job_1453815382601_0011 running in uber mode : false\n",
      "16/01/26 14:25:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 14:25:25 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/26 14:25:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/26 14:25:30 INFO mapreduce.Job: Job job_1453815382601_0011 completed successfully\n",
      "16/01/26 14:25:30 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2873\n",
      "\t\tFILE: Number of bytes written=359205\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216885\n",
      "\t\tHDFS: Number of bytes written=2663\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8612\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2807\n",
      "\t\tTotal time spent by all map tasks (ms)=8612\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2807\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8612\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2807\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8818688\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2874368\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=102\n",
      "\t\tMap output bytes=2663\n",
      "\t\tMap output materialized bytes=2879\n",
      "\t\tInput split bytes=206\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=101\n",
      "\t\tReduce shuffle bytes=2879\n",
      "\t\tReduce input records=102\n",
      "\t\tReduce output records=102\n",
      "\t\tSpilled Records=204\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=120\n",
      "\t\tCPU time spent (ms)=1630\n",
      "\t\tPhysical memory (bytes) snapshot=716083200\n",
      "\t\tVirtual memory (bytes) snapshot=2505347072\n",
      "\t\tTotal committed heap usage (bytes)=556269568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2663\n",
      "16/01/26 14:25:30 INFO streaming.StreamJob: Output directory: /user/ubuntu/NBOutputDrop3Stage2\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper \"/home/ubuntu/repos/261/HW2/mapper.py /home/ubuntu/repos/261/HW2/probs.txt\" \\\n",
    "-file /home/ubuntu/repos/261/HW2/probs.txt \\\n",
    "-input /user/ubuntu/enronemail_1h.txt \\\n",
    "-output /user/ubuntu/NBOutputDrop3Stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\r\n",
      "\t\r\n",
      "0001.1999-12-10.farmer\t0\t1\r\n",
      "0001.1999-12-10.kaminski\t0\t1\r\n",
      "0001.2000-01-17.beck\t0\t1\r\n",
      "0001.2001-02-07.kitchen\t0\t1\r\n",
      "0001.2001-04-02.williams\t0\t1\r\n",
      "0002.1999-12-13.farmer\t0\t1\r\n",
      "0002.2001-02-07.kitchen\t0\t1\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t1\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t1\r\n",
      "0003.2001-02-08.kitchen\t0\t1\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t1\r\n",
      "0004.2001-04-02.williams\t0\t1\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t1\r\n",
      "0005.2000-06-06.lokay\t0\t1\r\n",
      "0005.2001-02-08.kitchen\t0\t1\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t1\r\n",
      "0006.2001-02-08.kitchen\t0\t1\r\n",
      "0006.2001-04-03.williams\t0\t1\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t1\r\n",
      "0007.1999-12-14.farmer\t0\t1\r\n",
      "0007.2000-01-17.beck\t0\t1\r\n",
      "0007.2001-02-09.kitchen\t0\t1\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t1\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t1\r\n",
      "0009.1999-12-14.farmer\t0\t1\r\n",
      "0009.2000-06-07.lokay\t0\t1\r\n",
      "0009.2001-02-09.kitchen\t0\t1\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t1\r\n",
      "0010.1999-12-14.kaminski\t0\t1\r\n",
      "0010.2001-02-09.kitchen\t0\t1\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t1\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t1\r\n",
      "0012.1999-12-14.kaminski\t0\t1\r\n",
      "0012.2000-01-17.beck\t0\t1\r\n",
      "0012.2000-06-08.lokay\t0\t1\r\n",
      "0012.2001-02-09.kitchen\t0\t1\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t1\r\n",
      "0013.1999-12-14.kaminski\t0\t1\r\n",
      "0013.2001-04-03.williams\t0\t1\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t1\r\n",
      "0014.1999-12-15.farmer\t0\t1\r\n",
      "0014.2001-02-12.kitchen\t0\t1\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t1\r\n",
      "0015.1999-12-15.farmer\t0\t1\r\n",
      "0015.2000-06-09.lokay\t0\t1\r\n",
      "0015.2001-02-12.kitchen\t0\t1\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t1\r\n",
      "0016.2001-02-12.kitchen\t0\t1\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t1\r\n",
      "0017.2000-01-17.beck\t0\t1\r\n",
      "0017.2001-04-03.williams\t0\t1\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t1\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accuracy: 0.409836065574\t\r\n",
      "Accuracy: 0.475\t\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop/bin/hdfs dfs -cat /user/ubuntu/NBOutputDrop3Stage2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is much lower.  This is because there are a large number of words that appear once or twice but only in one class allowing for perfect prediction based solely on those words.  Ommitting them loses their predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.6\n",
    "Benchmark against SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '0' '0' '0' '0' '0' '1' '1' '1' '0' '0' '0' '0' '1' '1' '0' '0'\n",
      " '0' '1' '1' '0' '0' '0' '0' '1' '1' '0' '0' '0' '1' '1' '1' '0' '0' '0'\n",
      " '0' '1' '1' '0' '1' '1' '1' '1' '0' '0' '0' '0' '1' '0' '0' '0' '1' '1'\n",
      " '1' '0' '1' '1' '1' '1' '0' '0' '0' '0' '0' '1' '0' '0' '0' '1' '1' '0'\n",
      " '0' '0' '1' '1' '1' '0' '0' '0' '0' '1' '1' '0' '0' '1' '1' '1' '1' '0'\n",
      " '0' '0' '1' '1' '1' '0' '1' '1']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "import re\n",
    "emailfile = open(\"enronemail_1h.txt\", 'r')\n",
    "\n",
    "IDs = []\n",
    "labels = []\n",
    "contents = []\n",
    "\n",
    "for line in emailfile.readlines():\n",
    "    splits = re.split('\\t', line)\n",
    "    if len(splits) < 4:\n",
    "        continue\n",
    "    IDs.append(splits[0])\n",
    "    labels.append(splits[1])\n",
    "    contents.append(splits[2] + \" \" + splits[3])\n",
    "    \n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(contents)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(vectors, labels)\n",
    "pred = clf.predict(vectors)\n",
    "print(pred)\n",
    "correct_guesses = 0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == labels[i]:\n",
    "        correct_guesses += 1\n",
    "        \n",
    "print(1.*correct_guesses/len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn is 100% accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
